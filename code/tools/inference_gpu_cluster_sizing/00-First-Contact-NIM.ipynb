{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30afe4d4",
   "metadata": {},
   "source": [
    "# <font color=\"#76b900\">**Notebook 0:** First Contact With NIMs</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a43b9a",
   "metadata": {},
   "source": [
    "**Welcome to the introductory notebook on NVIDIA Inference Microservices (NIMs).** In this notebook, we will explore how to interact with a NIM endpoint, specifically focusing on the Llama 3 8B model. By the end of this notebook, you will have a foundational understanding of NIMs and how to perform basic tasks such as checking the status of a NIM endpoint, querying available models, and generating text.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "- Interact with a NIM endpoint to check its status and the available models.\n",
    "- Call a NIM endpoint of Llama 3 8B to generate text using curl and Python.\n",
    "- Understand the difference between end-to-end latency and time to first token.\n",
    "\n",
    "**Before starting this notebook, please make sure to watch its corresponding video.**\n",
    "\n",
    "## Table of Contents\n",
    "- [**Getting Started With Your First NIM**](#Getting-Started-With-Your-First-NIM)\n",
    "- [**End-To-End Latency versus Time-to-First-Token**](#End-To-End-Latency-versus-Time-to-First-Token)\n",
    "- [**[EXERCISE] Experimental Setup**](#[EXERCISE]-Experimental-Setup)\n",
    "- [**Next Steps**](#Next-Steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8a67aa",
   "metadata": {},
   "source": [
    "<br><hr>\n",
    "\n",
    "## **Getting Started With Your First NIM**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f33fc6a",
   "metadata": {},
   "source": [
    "[**NVIDIA NIMs**](https://www.nvidia.com/en-us/ai/) are microservices that kickstart GPU-optimized processes and conform to your system requirements for optimized delivery. They can be tested at [**build.nvidia.com**](https://www.build.nvidia.com) and span single-LLM (Llama, Mixtral, Nemotron, ...), non-linguistic models (diffusion, vision, health, ...), and orchestration (retriever) configurations. Additionally, they can be accessed via [**NGC**](https://catalog.ngc.nvidia.com) and kickstarted with relative ease. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a308b2",
   "metadata": {},
   "source": [
    "- **To get started with this course:** A NIM has already been kickstarted for you and will be up soon! Feel free to check out [**99-Reading-Logs.ipynb**](./99-Reading-Logs.ipynb) to check on its status. To see more about how this particular instance was deployed, check out [**`composer/docker-compose.yml`**](composer/docker-compose.yml).\n",
    "- **To learn about how to set one up with regular docker access:** Please visit [**99-Deploy-Llama-NIM.ipynb**](./99-Deploy-Llama-NIM.ipynb) to see how you could take advantage of NIMs using base Docker. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2effa6",
   "metadata": {},
   "source": [
    "Once the service is live, we can check that NIM is available as a `nim` service running in the background from another container. From the host machine, you should change the URL from `nim` to `localhost`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a12a9177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"object\":\"health.response\",\"message\":\"Service is ready.\"}"
     ]
    }
   ],
   "source": [
    "!curl http://nim:8000/v1/health/ready"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1a0d67",
   "metadata": {},
   "source": [
    "Assuming that the microservice is ready, we should get a response similar to `{\"object\": \"health-response\", \"message\": \"Service is ready.\"}`. If you do not see this, check back on your deployment in [**99-Deploy-Llama-NIM.ipynb**](./99-Deploy-Llama-NIM.ipynb) and see if something went wrong.\n",
    "\n",
    "Next, let's verify the model loaded into NIM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42d03a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta/llama3-8b-instruct\n"
     ]
    }
   ],
   "source": [
    "!curl -s -X GET 'http://nim:8000/v1/models' | jq -r '.data[0].id'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc107a7",
   "metadata": {},
   "source": [
    "You should see that `meta/llama3-8b-instruct` is available, and we will call it to generate responses. Let's test it by performing a simple inference request via its most basic interface; a direct curl request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1845f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A Graphics Processing Unit (GPU) is a specialized electronic circuit designed to quickly manipulate and alter memory to accelerate the creation of images in a frame buffer intended\n",
      "CPU times: user 1.47 ms, sys: 4.15 ms, total: 5.63 ms\n",
      "Wall time: 436 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%bash\n",
    "# OPTION 1: Send requests in terminal via `curl`\n",
    "curl -s -X 'POST' \\\n",
    "    'http://nim:8000/v1/completions' \\\n",
    "    -H 'accept: application/json' \\\n",
    "    -H 'Content-Type: application/json' \\\n",
    "    -d '{\n",
    "\"model\": \"meta/llama3-8b-instruct\",\n",
    "\"prompt\": \"Could you explain what a GPU is?\",\n",
    "\"max_tokens\": 30\n",
    "}' | jq -r '.choices[0].text'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a029c2",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Feel free to modify the prompt of the number of max_tokens to produce different text. When increasing the number of max_tokens, you will notice that the response takes longer. For example, let's increase the number of max_tokens to 300 and time the request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91537295-bc5a-449c-a314-2c1ce10b5939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "A GPU is abbreviated as Graphics Processing Unit. It is a crucial component of a computer system, responsible for handling and rendering graphical and computationally intensive tasks. In this post, you will learn about what a GPU is, its role in computers, and why it is essential for many modern applications.\n",
      "\n",
      "What is a GPU?\n",
      "----------------\n",
      "\n",
      "A GPU (Graphics Processing Unit) is a specialized electronic circuit designed to quickly manipulate and alter memory to accelerate the creation of images in a frame buffer intended for output to a display device. Graphics Processing Units have evolved to become highly parallel processing units, with thousands of processing cores, capable of executing many instructions simultaneously.\n",
      "\n",
      "Architecture of a GPU\n",
      "-----------------\n",
      "\n",
      "GPUs are typically comprised of multiple components, including:\n",
      "\n",
      "1. **CUDA Cores or Stream Processors**: These are the primary processing units responsible for performing calculations. The number of CUDA cores or stream processors varies depending on the GPU model and manufacturer.\n",
      "2. **Memory Hierarchy**: GPUs have a hierarchical memory structure, which includes cache memory, video random-access memory (VRAM), and system memory. This hierarchy enables fast access to frequently used data.\n",
      "3. **Memory Interface**: The memory interface handles data transfer between the GPU and system memory.\n",
      "4. **Raster Operations Pipeline**: This pipeline is responsible for transforming 2D graphics commands into pixel data and transferring it to the frame buffer.\n",
      "\n",
      "Role of a GPU\n",
      "--------------\n",
      "\n",
      "GPUs are designed to accelerate two primary types of tasks:\n",
      "\n",
      "1. **Graphics Processing**:\n",
      "CPU times: user 56.5 ms, sys: 4.45 ms, total: 60.9 ms\n",
      "Wall time: 3.94 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## OPTION 1: continue with `curl`\n",
    "# %%bash\n",
    "# TIMEFORMAT=\"%Es\" curl -s -X 'POST' \\ -s -X 'POST' \\\n",
    "#     'http://nim:8000/v1/completions' \\\n",
    "#     -H 'accept: application/json' \\\n",
    "#     -H 'Content-Type: application/json' \\\n",
    "#     -d '{\n",
    "# \"model\": \"meta/llama3-8b-instruct\",\n",
    "# \"prompt\": \"Could you explain what a GPU is?\",\n",
    "# \"max_tokens\": 300\n",
    "# }' | jq -r '.choices[0].text'\n",
    "\n",
    "## Option 2: Continue in python via `requests`\n",
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"http://nim:8000/v1/completions\"\n",
    "headers = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "data = {\n",
    "    \"model\": \"meta/llama3-8b-instruct\",\n",
    "    \"prompt\": \"Could you explain what a GPU is?\",\n",
    "    \"max_tokens\": 300,\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "\n",
    "if response.status_code == 200:\n",
    "    response_text = response.json().get('choices', [{}])[0].get('text', '').strip()\n",
    "    print(f\"Response:\\n{response_text}\")\n",
    "else:\n",
    "    print(f\"Failed to get a response, status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea16f14c",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "In our setup, the longer response takes around 3.2s, vs 0.6s for the shorter one. Note that the `max_tokens` number of tokens may not be reached if an end-of-sequence token is returned by the LLM before. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2f2da7-50ed-4730-8446-7a658ef33e3c",
   "metadata": {},
   "source": [
    "<br><hr>\n",
    "\n",
    "## **End-To-End Latency versus Time-to-First-Token**\n",
    "\n",
    "An inference request has two main stages: **prefill** and **decoding**.\n",
    "- During **prefill**, the LLM processes the prompt we send to the model, which in our example was \"Could you explain what a GPU is?\", and produces the first generated token.\n",
    "    - `TTFT`: **time-to-first-token**, or prefill duration.\n",
    "- During **decoding**, the LLM predicts the subsequent tokens one at a time, until reaching max_tokens or producing an end-of-sequence token. Prefill and decoding phases happen internally even if only the completed response is returned, as above.\n",
    "    - `E2E Latency`: **End-to-end latency** from the combined prefill and decoding stages.\n",
    "\n",
    "The latency you measured before was the `E2E Latency`, but it will be better for us to focus on `TTFT` for streaming applications like chatbots. Since the user can start reading the response as the LLM generates tokens, the user experience is acceptable as long as the speed of token generation is faster than the human reading speed. \n",
    "\n",
    "Below, we connect to our NIM microservice via the OpenAI client for simplicity, swapping the `base_url` to our local endpoint and setting `stream=True` to invoke streaming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea2ef592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was the early 1990s, and the world of computing was on the cusp of a revolution. The once-dominant CPU (Central Processing Unit) was about to meet its match in a new kind of processor: the GPU (Graphics Processing Unit).\n",
      "\n",
      "In those days, the graphics capabilities of computers were limited to simple, 2D graphics. But a small startup called NVIDIA, founded by Jensen Huang and Chris Malachowsky, had a vision to change all that. Their goal was to create a processor that could handle complex, 3D graphics with ease.\n",
      "\n",
      "The first generation of NVIDIA's GPUs, the RIVA 128, was released in 1997. It was a game-changer. No longer would gamers and graphics enthusiasts have to settle for bland, 2D graphics. The RIVA 128 brought 3D rendering to the masses, and with it, a whole new world of possibilities opened up.\n",
      "\n",
      "As the years went by, NVIDIA continued to push the boundaries of what was possible with GPUs. They introduced new architectures like the GeForce and the Quadro, each designed to handle more complex graphics tasks. The company's innovations didn't go unnoticed, and soon, GPUs were being used in a wide range of applications beyond gaming: scientific visualization, video editing, and even cryptocurrency mining.\n",
      "\n",
      "But NVIDIA wasn't the only player in the GPU game. In the early 2000s, a company called ATI (Advanced Technology Incorporated) emerged as a major"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import time\n",
    "import sys\n",
    "\n",
    "client = OpenAI(base_url=\"http://nim:8000/v1\", api_key=\"not-used\")\n",
    "response = client.chat.completions.create(\n",
    "    model=\"meta/llama3-8b-instruct\",\n",
    "    messages=[\n",
    "        {'role': 'user', 'content': 'Tell me a long story about GPUs'}\n",
    "    ],\n",
    "    max_tokens=300,\n",
    "    stream=True\n",
    ")\n",
    "    \n",
    "for chunk in response:\n",
    "    content = chunk.choices[0].delta.content\n",
    "    if content is not None:\n",
    "        sys.stdout.write(content)\n",
    "        sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e7583e",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "You can see that a user could enjoy a decent reading experience even as the model generates more and more of the response. This resembles the experience of interacting with an online LLM inference application like a chatbot, which explains why `TTFT` is so important there.\n",
    "\n",
    "Notice that our prompt was a very short \"Tell me a long story about GPUs\" request. As the length prompt increases, we should expect the `TTFT` to go up since more and more context tokens have to be propagated through our network. Perhaps the prompt length should be a factor in our timing considerations..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab48d64f-dc61-4086-9584-402a5a668663",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## **[EXERCISE] Experimental Setup**\n",
    "\n",
    "To reinforce this point, let's modify the previous code to streamline experimentation. Please define the `measure_latency` command below to help you perform some simple timing experiments based on a set of reasonable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "823ed1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def measure_latency(n_input_tokens: int = 50, max_output_tokens: int = 1, verbose = True):\n",
    "    # Let's define a dummy prompt with a variable number of input tokens\n",
    "    dummy_prompt = \" \".join([\"hi\"] * (n_input_tokens - 1))\n",
    "\n",
    "    ## TODO: Create a connector that connects to the nim endpoint\n",
    "    client = None\n",
    "    \n",
    "    # Record the start time and (later) the end time of the simulation to compute duration\n",
    "    start_time = time.time()\n",
    "    \n",
    "    ## TODO: Using client, send the request to the NIM\n",
    "    ## - Make sure to set max_tokens to stop generation\n",
    "    ## - Make sure stream=True as the following code expects a generator\n",
    "    ## - Set messages=[...] with the openai library\n",
    "    ##    - But if you notice you're not getting enough outputs, use \"prompt\": \"...\"  with the requests library\n",
    "    response = None\n",
    "    \n",
    "    ## Wait for the responses to come in, accumulating them along the way\n",
    "    ## NOTE: If using Chat endpoint, the first token is generally a confirmation empty-token. Best solution is to check if token has content\n",
    "    n_generated_tokens = 0\n",
    "    # n_generated_tokens = -1\n",
    "    for chunk in response:\n",
    "        n_generated_tokens += 1\n",
    "    duration = time.time() - start_time\n",
    "    \n",
    "    if verbose: \n",
    "        print(f\"{n_generated_tokens}-token latency with {n_input_tokens} input tokens is {duration:.2f}s\")\n",
    "    \n",
    "    return duration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c497ede",
   "metadata": {},
   "source": [
    "The previous code measures the time to generate a response according to the specified `n_input_tokens` and `max_output_tokens`. We are not printing the response since we employ a dummy input prompt formed by repeated \"hi\"s. Note that the latency of the LLM doesn't depend on the type of token passed into it: as a result, it doesn't really matter what tokens are contained in the prompt. For latency purposes, the only important variable is the number of tokens in the prompt.\n",
    "Let's time the function call with the default `max_output_tokens=1` to measure the `TTFT`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efb7d822",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Measure the latency of 50-in, 1-out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2f234d",
   "metadata": {},
   "source": [
    "As the number of input tokens increases, so does the `TTFT`. For example, let's set 8000 input tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7874d21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Measure the latency of 8000-in, 1-out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8695c39c",
   "metadata": {},
   "source": [
    "In our setup, the `TTFT` increased from 0.03s to 0.6s, which isn't too terrible but still a sizable increase. Now, consider what happens when we try to generate 500 tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1e87212",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Measure the latency of 50-in, 500-out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe657b8",
   "metadata": {},
   "source": [
    "**In our setup, assuming no extraneous load from other services/users:**\n",
    " - The `TTFT` for just 50 input tokens takes ~0.02s. There's very little prefill to process and very little decoding to do, so this makes a lot of sense. \n",
    " - The `TTFT` for 8000 input tokens takes ~0.6s. This is quite a reasonable number for user experience, but the impact is sizable.\n",
    " - The `E2E Latency` of just 50 input tokens but 500 output tokens takes ~6.6s. This is because the prefill stage is more efficient than decoding. This will be discussed in detail in the next notebook but has to do with the one-at-a-time nature of autoregressive decoding.\n",
    "\n",
    "That's why streaming is so powerful in improving user experience. We encourage you to try measuring the `TTFT` and `E2E Latency` for other use cases by changing `n_input_tokens` and `max_output_tokens` in the function `measure_latency()`.\n",
    "\n",
    "<details>\n",
    "<summary><b>Reveal Solution</b></summary>\n",
    "\n",
    "```python \n",
    "import time\n",
    "\n",
    "def measure_latency(n_input_tokens: int = 50, max_output_tokens: int = 1, verbose = True):\n",
    "    # Let's define a dummy prompt with a variable number of input tokens\n",
    "    dummy_prompt = \" \".join([\"hi\"] * (n_input_tokens - 1))\n",
    "\n",
    "    ## TODO: Create a connector that connects to the nim endpoint\n",
    "    client = OpenAI(base_url=\"http://nim:8000/v1\", api_key=\"not-used\")\n",
    "    \n",
    "    # Record the start time and (later) the end time of the simulation to compute duration\n",
    "    start_time = time.time()\n",
    "    \n",
    "    ## TODO: Using client, send the request to the NIM\n",
    "    ## - Make sure to set max_tokens to stop generation\n",
    "    ## - Make sure stream=True as the following code expects a generator\n",
    "    ## - Set messages=[...] with the openai library\n",
    "    ##    - But if you encounter errors, use \"prompt\": \"...\"  with the requests library\n",
    "    # response = client.chat.completions.create(\n",
    "    response = client.completions.create(\n",
    "        model=\"meta/llama3-8b-instruct\",\n",
    "        # messages = [{'role': 'user', 'content': dummy_prompt}],\n",
    "        prompt = dummy_prompt,\n",
    "        max_tokens = max_output_tokens,\n",
    "        stream = True,\n",
    "    )\n",
    "    ## NOTE: With messages, models have strong priors to stop generating text \"in a conversational fashion\"\n",
    "    ## As such, trying to use messages here will result in shorter generations which will mess up the benchmarks.\n",
    "    ## Later notebooks address this issue by explicitly disabling end-of-string tokens under the hood. \n",
    "\n",
    "    ## Wait for the responses to come in, accumulating them along the way\n",
    "    ## NOTE: If using Chat endpoint, the first token is generally a confirmation empty-token. Best solution is to check if token has content\n",
    "    n_generated_tokens = 0\n",
    "    # n_generated_tokens = -1\n",
    "    for chunk in response:\n",
    "        n_generated_tokens += 1\n",
    "    duration = time.time() - start_time\n",
    "    \n",
    "    if verbose: \n",
    "        print(f\"{n_generated_tokens}-token latency with {n_input_tokens} input tokens is {duration:.2f}s\")\n",
    "    \n",
    "    return duration\n",
    "\n",
    "## TODO: Measure the latency of 50-in, 1-out\n",
    "measure_latency(50);\n",
    "\n",
    "## TODO: Measure the latency of 8000-in, 1-out\n",
    "measure_latency(8000);\n",
    "\n",
    "## TODO: Measure the latency of 50-in, 500-out\n",
    "measure_latency(50, 500);\n",
    "```\n",
    "\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
