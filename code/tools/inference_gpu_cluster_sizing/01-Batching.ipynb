{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7eb82c3b",
   "metadata": {},
   "source": [
    "# <font color=\"#76b900\">**Notebook 1:** Understanding Batching Strategies</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a80c7d-606f-443a-a8a4-51c1ffff129f",
   "metadata": {},
   "source": [
    "Welcome back. In this notebook you will dive deeper into the metrics, characterizing the speed of the LLM inference engine. You will explore the cutting-edge optimizations employed in modern inference engines, simulate their effects, and analyze the impact on the key metrics that matter most.\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this notebook, you will be able to:\n",
    "- Understand and measure time to first token (TTFT), end-to-end latency (E2E Latency), and inter-token latency (ITL).\n",
    "- Analyze throughput metrics and simulate their dependencies on various factors.\n",
    "- Explore the impact of batching and inflight batching on GPU utilization and performance.\n",
    "- Investigate the effects of concurrency settings on latency and throughput.\n",
    "\n",
    "**Before starting this notebook, please make sure to watch its corresponding video.**\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "- [**Latency Metrics**](#Latency-Metrics)\n",
    "  - [[EXCERCISE] Estimating Inter-Token Latency (ITL) For Human Perception](#[EXERCISE]-Estimating-Inter-Token-Latency-(ITL)-For-Human-Perception)\n",
    "- [**Introducing the Simulator**](#Introducing-the-Simulator)\n",
    "- [**Batching: The Key to Efficient Throughput**](#Batching:-The-Key-to-Efficient-Throughput)\n",
    "  - [Memory-Bound and Compute-Bound Functions](#Memory-Bound-and-Compute-Bound-Functions)\n",
    "  - [**Prefill:** A Compute-Bound Operation](#Prefill:-A-Compute-Bound-Operation)\n",
    "  - [**Decoding:** A Memory-Bound Operation](#Decoding:-A-Memory-Bound-Operation)\n",
    "  - [Arithmetic Intensity](#Arithmetic-Intensity)\n",
    "  - [Simulating Infinitely Compute-Bound Prefill and Infinitely Memory-Bound Decoding](#Simulating-Infinitely-Compute-Bound-Prefill-and-Infinitely-Memory-Bound-Decoding)\n",
    "  - [Adding Variable Output Length](#Adding-Variable-Output-Length)\n",
    "- [**Throughput Metrics**](#Throughput-Metrics)\n",
    "  - [Choosing The Right Throughput Metric](#Choosing-The-Right-Throughput-Metric)\n",
    "  - [[EXCERCISE] Implement Metrics Printing and Throughput Measurement In the Simulation](#[EXERCISE]-Measuring-Throughput-and-Printing-Metrics)\n",
    "- [**Inflight Batching (IFB)**](#Inflight-Batching-(IFB))\n",
    "  - [IFB in simulation](#IFB-In-Simulation)\n",
    "  - [Chunked Context](#Chunked-Context)\n",
    "  - [[EXCERCISE] Batcher With Only One Prefill Per Batch](#[EXCERCISE]-Batcher-With-Only-One-Prefill-Per-Batch)\n",
    "  - [Client-Side Concurrency](#Client-Side-Concurrency)\n",
    "  - [Max Batch Size](#Max-Batch-Size)\n",
    "- [**Additional Notes**](#Additional-Notes)\n",
    "  - [On concurrency and request rate as a result metric](#On-concurrency-and-request-rate-as-a-result-metric)\n",
    "  - [On concurrency and request rate as an input parameter](#On-concurrency-and-request-rate-as-an-input-parameter)\n",
    "  - [[OPTIONAL EXCERCISE] Queue Growth](#[OPTIONAL-EXCERCISE]-Queue-Growth)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2e82a2",
   "metadata": {},
   "source": [
    "<br><hr>\n",
    "\n",
    "## **Latency Metrics**\n",
    "Several important metrics are available in the benchmarks:\n",
    "\n",
    "* **TTFT (Time To First Token)**: measures the time it takes for the model to generate the first token of a response. You've experienced it in the previous notebook.\n",
    "* **E2E Latency (End-to-End Latency)**: measures the total time it takes for the model to generate a complete response.\n",
    "* **ITL (Inter-Token Latency)**: also known as Time Per Output Token (TPOT), measures the average time the client waits between consecutive tokens in a response in the streaming scenario.\n",
    "\n",
    "To separate the prefill characteristics from the decoding ones, TTFT and ITL are reported independently in GenAI-Perf, as shown in the image below.\n",
    "\n",
    "<img src=\"images/metrics.png\" alt=\"metrics\" width=1200px/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d287250",
   "metadata": {},
   "source": [
    "### **[EXERCISE]** Estimating Inter-Token Latency (ITL) For Human Perception\n",
    "\n",
    "Let us try to estimate typical inter-token latencies for human perception. Consider the following details:\n",
    "\n",
    "- Normal reading for comprehension speed is about 200–230 words per minute (wpm). \n",
    "- Skimming speed is 700 wpm ([see wiki](https://en.wikipedia.org/wiki/Speed_reading#Skimming_and_scanning)). \n",
    "- We can assume that an arbitrary token accounts for around 0.75 words, on average (as a standard simplifying assumption used a lot for English).\n",
    "\n",
    "Let's convert these to ITL-compatible units of ms/token. Using reasonable average input statistics, we can expect to get decent average output statistics via some simple unit conversion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4cb1559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING:  230 words per minute correspond to 0ms between tokens, on average\n",
      "SKIMMING: 700 words per minute correspond to 0ms between tokens, on average\n"
     ]
    }
   ],
   "source": [
    "def calculate_itl(wpm, words_per_token=0.75):\n",
    "    ## TODO: Implement the method\n",
    "    \n",
    "    ## NOTE: 60 seconds to a minute, 1000 milliseconds to a second\n",
    "\n",
    "    ## NOTE: Unit arithmetic of [t/s] = [(words/min) * (tokens/word) * (min/s)] = [(words/min) / (words/tokens) / (s/min)]\n",
    "    tokens_per_second = 0\n",
    "\n",
    "    ## NOTE: Unit arithmetic of [ms/t] = [(ms/s * s/token)] = [(ms/s) / (token/s)]\n",
    "    inter_token_latency_ms = 0\n",
    "\n",
    "    return inter_token_latency_ms\n",
    "\n",
    "print(f\"READING:  230 words per minute correspond to {calculate_itl(230):.0f}ms between tokens, on average\")\n",
    "print(f\"SKIMMING: 700 words per minute correspond to {calculate_itl(700):.0f}ms between tokens, on average\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a24d76",
   "metadata": {},
   "source": [
    "In the vast majority of the LLM inference setups, ITL is much lower than these reference values, typically around 20–40 ms. Still, it's important to keep these numbers in mind as minimum thresholds for comfort.\n",
    "\n",
    "<details>\n",
    "<summary><b>Reveal Solution</b></summary>\n",
    "\n",
    "```python \n",
    "def calculate_itl(wpm, words_per_token=0.75):\n",
    "    ## NOTE: 60 seconds to a minute, 1000 milliseconds to a second\n",
    "\n",
    "    ## NOTE: Unit arithmetic of [t/s] = [(words/min) * (tokens/word) * (min/s)] = [(words/min) / (words/tokens) / (s/min)]\n",
    "    tokens_per_second = wpm / words_per_token / 60\n",
    "\n",
    "    ## NOTE: Unit arithmetic of [ms/t] = [(ms/s * s/token)] = [(ms/s) / (token/s)]\n",
    "    inter_token_latency_ms = 1000 / tokens_per_second\n",
    "\n",
    "    return inter_token_latency_ms\n",
    "\n",
    "print(f\"READING:  230 words per minute correspond to {calculate_itl(230):.0f}ms between tokens, on average\")  ## 196\n",
    "print(f\"SKIMMING: 700 words per minute correspond to {calculate_itl(700):.0f}ms between tokens, on average\")  ## 64\n",
    "```\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "**NOTES:** \n",
    "- In addition to defining a minimum threshold for comfort, you may also find it useful to identify a maximum threshold for speed in certain contexts. For example, a chat application with a lower-than-usual load might be so fast that it dumps output at an uncomfortable rate and causes the text view to scroll automatically, making it uncomfortable to read. For these cases, artificially sleeping between or iterating over streaming yields may be desirable. \n",
    "- [**LLMPerf**](https://github.com/ray-project/llmperf), another common benchmarking SW, incorporates first-token latency into inter-token latency computation. Beware of comparing directly the results from different benchmarking tools."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8dbd80c",
   "metadata": {},
   "source": [
    "<hr><br>\n",
    "\n",
    "## **Introducing the Simulator**\n",
    "\n",
    "In this notebook, you will leverage and further develop a simulator that models how Tensor-LLM assembles the requests into batches. For now, let's use the default parameters and review the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4a7b151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"620\"\n",
       "    src=\"iframe_figures/figure_2.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.io as pio\n",
    "pio.renderers.default = \"iframe\"\n",
    "\n",
    "import simulator as sim\n",
    "import numpy as np\n",
    "\n",
    "engine = sim.Engine(\n",
    "    max_batch_size = 2, # creates two slots in the batch\n",
    "    load_generator = sim.BatchLoadGenerator(initial_batch=1), # it sends only one request at engine.current_time == 0\n",
    "    batcher = sim.StaticBatcher()\n",
    ")\n",
    "engine.run(time_limit=10)\n",
    "engine.plot_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32783e2",
   "metadata": {},
   "source": [
    "You see two plots above. In the top plot, you can see batch composition depending on time. Each column is one LLM evaluation from the first layers to the last. The color of the cell and the letter represent the current stage of the request in the batch:\n",
    "* **p**refill (pink)\n",
    "* **d**ecoding (yellow) \n",
    "* **e**mpty slot (blue)\n",
    "\n",
    "The width of the **p**refill cell is equal to the TTFT ($=2$) and the width of the **d**ecoding cell corresponds to ITL ($=1$). Make sure to hover your mouse over the cells to get detailed information about the execution status, including width in ticks.\n",
    "\n",
    "In the bottom plot, you can see the measured latencies at the moments of measurement. For example, the pink $(2, 2)$ point represents the TTFT measurement of our request: at the time of $2$ ticks the prefill has been completed and it took $2$ ticks for our request. Similarly, we represent the E2E latency of $5$ measured at $5$ ticks by the green-yellow point. Additionally, we show the current queue size by the orange line. Since we send only one request in this example, it is always equal to 0. Note that the latency scale uses the left y-axis and the queue size uses the right one. The x-axis is shared between both plots."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861cfae2",
   "metadata": {},
   "source": [
    "<br><hr>\n",
    "\n",
    "## **Batching: The Key to Efficient Throughput**\n",
    "\n",
    "GPUs are very good at processing highly-parallelized and concurrent tasks. For example, an NVIDIA H100 GPU has 16,896 FP32 Cores per GPU and 528 Tensor Cores organized into 132 streaming multiprocessors (SMs) ([**see the datasheet**](https://nvdam.widen.net/s/95bdhpsgrs/nvidia_h100_tensor_core_gpu_architecture_whitepaper_v1.03#page=39)), where each core can execute an independent thread of mathematical computation and each SM can parallelize hundreds of threads (either core-enabled math operations or parallelizable memory operations) at a time. The most efficient way to utilize the GPU is to make sure all the SMs always have something to compute and some memory operations to run at any given time.  \n",
    "\n",
    "### Memory-Bound and Compute-Bound Functions\n",
    "\n",
    "Now consider a simplified model where a function reads its input from memory, performs math operations, and then writes its output to memory. Let's assume $T_{mem}$ time is spent in accessing memory and $T_{math}$ time is spent performing math operations. If we further assume that memory and math portions of different threads can be overlapped: \n",
    "- The total time for the function is $max(T_{mem}, T_{math})$, and the longer of the two times demonstrates what limits performance.\n",
    "- If math time $T_{math}$ is longer we say that a function is `math limited` or `compute-bound`. \n",
    "- If memory time $T_{mem}$ is longer then it is `memory limited` or `memory-bound`. \n",
    "\n",
    "For more low-level details see [**NVIDIA Deep Learning Performance Documentation**](https://docs.nvidia.com/deeplearning/performance/index.html) and in particular [**GPU Performance Background User's Guide**](https://docs.nvidia.com/deeplearning/performance/dl-performance-gpu-background/index.html), but for now let's connect these topics to the LLM operations we take for granted in typical use.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0585bd0-1669-472f-8e09-4c7e9ef9b9e2",
   "metadata": {},
   "source": [
    "### **Prefill:** A Compute-Bound Operation\n",
    "\n",
    "**During prefill, most operations are compute-bound.**\n",
    "- Propagating the initial context requires larger matrices to interact to resolve attention across the entire prefill context.\n",
    "- The intermediate results of the calculation are written to KV-cache (cached attention matrix values stored in memory), but that requires few memory operations.\n",
    "- Compute-bound property generally manifests after a certain prefill token limit (in our tests, around 300 tokens or more cause $T_{math} > T_{mem}$ on our GPU setup).\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d2a3ab-5fe3-4ecc-910d-89c9c3090dd1",
   "metadata": {},
   "source": [
    "### **Decoding:** A Memory-Bound Operation\n",
    "\n",
    "**During decoding, most operations are memory-bound.**\n",
    "- Generating one token at a time means the input for each next token is a single embedded token and many cached components, which results in small matrix operations during forward propagation. \n",
    "- The KV-cache from previously provided/generated tokens continues to grow, so retrieving requires more resources. \n",
    "\n",
    "To improve the efficiency, you need to increase computations per read byte of memory. The simplest way to do it is by batching the requests together. With the batch size `b` the system can load the weights of the LLM from the GPU memory to the SMs once, but compute `b` next tokens.\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0f7233",
   "metadata": {},
   "source": [
    "### Arithmetic Intensity\n",
    "\n",
    "To help support this model, **arithmetic intensity** is a common metric for evaluating the compute-boundedness of a given function. It is defined as the ratio of floating-point operations to the number of data elements accessed by the function - usually in FLOPs/byte - with a high arithmetic intensity indicating a high computational load. \n",
    "\n",
    "Below is a figure from the paper [**SARATHI: Efficient LLM Inference by Piggybacking Decodes with Chunked Prefills**](https://arxiv.org/pdf/2308.16369) demonstrating the arithmetic intensity of prefills and decodes for LLaMA-13B on A6000 GPU. The different colors represent different operations within the transformer block: *preproj* for preprojection, a single matrix multiplication; *attn* for attention computation, *preproj* for postprojection, and *ffn* for feed-forward network.\n",
    "\n",
    "<img src=\"images/sarathi.png\" alt=\"Arithmetic Intensity\" width=1200px/>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39f558a",
   "metadata": {},
   "source": [
    "### Simulating Infinitely Compute-Bound Prefill and Infinitely Memory-Bound Decoding\n",
    "\n",
    "Using the simulator, we can model our properties of interest to their extremes and see how our system performs in asymptotic cases. \n",
    "* Batching together $N$ prefills in one slot takes $N\\times$ the time.\n",
    "* Batching together as many decodings won't affect the ITL. \n",
    "\n",
    "Optional: Feel free to uncomment the next line and review the code of duration estimation. Note that we sum up the $T_{math}$ from prefills and compute max between $T_{mem}$ from decodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ca0dd54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0msim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEngine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_current_batch_duration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m <no docstring>\n",
       "\u001b[0;31mSource:\u001b[0m   \n",
       "    \u001b[0;32mdef\u001b[0m \u001b[0mget_current_batch_duration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mdecoding_requests\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_decoding_requests\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;31m# for no chunking the line below is equivalent to\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;31m# prefill_time = sum([req.prefill_time for req in self.get_prefilling_requests()])\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mprefill_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_current_duration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mreq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_prefilling_requests\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mitl_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitl\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mreq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdecoding_requests\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdecoding_requests\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefill_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitl_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 1. is the minimal step duration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m      /dli/task/simulator/engine.py\n",
       "\u001b[0;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sim.Engine.get_current_batch_duration??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b770d086",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Let's now simulate 2 concurrent requests, submitted at time 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db3053b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"620\"\n",
       "    src=\"iframe_figures/figure_4.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "engine = sim.Engine(\n",
    "    max_batch_size = 2, # creates two slots in the batch\n",
    "    load_generator = sim.BatchLoadGenerator(initial_batch=2), # it sends 2 requests at engine.current_time == 0\n",
    "    batcher = sim.StaticBatcher()\n",
    ")\n",
    "engine.run(time_limit=10)\n",
    "engine.plot_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33e5da2",
   "metadata": {},
   "source": [
    "As expected, now both requests have TTFT of $2*2=4$ but the ITL is still $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babcd83f",
   "metadata": {},
   "source": [
    "### Adding Variable Output Length\n",
    "Now let's increase our output length to $10$ tokens and set its standard deviation to $5$ to simulate variability in the expected response lengths. We also increase simulation `time_limit` to $100$ and `max_batch_size` to $4$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9152d91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"620\"\n",
       "    src=\"iframe_figures/figure_5.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "load_generator = sim.BatchLoadGenerator( \n",
    "    initial_batch=100, # how many requests it sends at engine.current_time == 0\n",
    "    target_output_len_tokens=10,\n",
    ")\n",
    "load_generator.target_output_len_std = 5\n",
    "\n",
    "engine = sim.Engine(\n",
    "    max_batch_size = 4, # how many slots in the batch\n",
    "    load_generator=load_generator,\n",
    "    batcher = sim.StaticBatcher()\n",
    ")\n",
    "engine.run(time_limit=100)\n",
    "engine.plot_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c7430c",
   "metadata": {},
   "source": [
    "When running this, you can see that there are a lot of **e**mpty slots which indicate that the GPUs aren't utilized efficiently. But how do we measure this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fe7c67",
   "metadata": {},
   "source": [
    "<br><hr>\n",
    "\n",
    "## **Throughput Metrics**\n",
    "\n",
    "One of the advantages of using latency metrics is their ease of interpretation and lack of ambiguity. They can be measured regardless of the inference system's organization, and they require no awkward normalization to become immediately interpretable! \n",
    "\n",
    "However, to get from latency benchmarks to GPU count estimations, one also needs some throughput metrics. These metrics measure the capacity of a system to process data per unit of time. Here's a breakdown of the various throughput metrics and their implications:\n",
    "\n",
    "- **Tokens per second per model instance:**\n",
    "    - **Across all phases:** Measures the total processing capability of a single model instance, including pre-processing, generation, and post-processing stages.\n",
    "    - **Only in the generation phase (a.k.a 1/ITL):** Focuses on the model's ability to generate tokens, offering a direct measure of the model’s generative performance.\n",
    "\n",
    "- **Tokens per second per GPU:**\n",
    "    - This metric can be specified for either only the generation phase or all phases, indicating how effectively a GPU is being utilized to process tokens. This helps in assessing the efficiency of the GPU in handling specific tasks within the inference pipeline.\n",
    "\n",
    "- **Tokens per second per server:**\n",
    "    - Similar to the per GPU metric but scaled up to the server level. This measures the overall throughput of an entire server, which may contain multiple GPUs and model instances. It's crucial for evaluating server-level performance and infrastructure scalability.\n",
    "\n",
    "- **Prompts per second:**\n",
    "    - **Per Model Instance:** This measures how many complete prompts a single model instance can handle in one second, providing a straightforward metric of model instance efficiency.\n",
    "    - **Per GPU:** Reflects the number of prompts a GPU can process per second, useful for gauging GPU performance in a real-world application scenario.\n",
    "    - **Per Server:** Measures the capacity of a server to handle prompts, indicating the throughput at the server scale.\n",
    "\n",
    "- **Concurrent Metrics:**\n",
    "    - **Concurrent Requests:** Refers to the number of requests a system can handle at the same time. It's a critical measure of system robustness and concurrency handling.\n",
    "    - **Concurrent Clients:** Indicates how many clients can simultaneously interact with the system without degrading performance, essential for understanding the scalability of client-server interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29342424-458b-429b-b1bf-43e3f83a7097",
   "metadata": {},
   "source": [
    "### Choosing The Right Throughput Metric\n",
    "\n",
    "Often, benchmarking software shortens the units to just `tokens/second`, leading to ambiguity about the applied normalization. For the purposes of sizing, the most convenient throughput metric is `prompts/second/server`, which allows benchmarkers to choose between different combinations of tensor parallelism strategies (from now on `TP`) with the number of servers being a natural parameter. In our established benchmarks, we normalize by the standard servers with 8 GPUs, meaning that we consider the throughput of 2 instances of TP4 or 8 instances of TP1. This metric also highlights the dependence of the throughput on the specific composition of requests, including input and output lengths."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e0d572-dd15-4973-b4df-cc7b912e3599",
   "metadata": {},
   "source": [
    "### **[EXERCISE]** Measuring Throughput and Printing Metrics\n",
    "\n",
    "In our simulation, we have an `engine.plot_data.metrics` object implemented in [./simulator/metrics.py](./simulator/metrics.py). Every time a new request completes, we record a pair of `(current time, E2E Latency)` to `metrics.e2e_latency`. The `metrics.get_e2e_latencies()` returns just the measured values. Complete the `print_experiment_metrics` function below to quantify the speedups. Please, don't change any formatting for now, so that the helper function can verify your implementation is correct, see the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65fe4ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Experiment Config:\n",
      "load_generator = BatchLoadGenerator(\n",
      "     prefill_time=2\n",
      "     itl=1\n",
      "     target_output_len_tokens=10\n",
      "     total_prefill_chunks=1\n",
      "     initial_batch=100\n",
      "     target_output_len_std=5\n",
      ")\n",
      "batcher = StaticBatcher\n",
      "\n",
      "# Latency Metrics:\n",
      "Average E2E Latency: 58.16\n",
      "Average TTFT: 52.80\n",
      "Average ITL: 1.00\n",
      "Median E2E Latency: 16.27\n",
      "Median TTFT: 8.00\n",
      "Median ITL: 1.00\n",
      "\n",
      "# Throughput Metrics:\n",
      "Requests/(1K ticks)/instance = -1.00\n",
      "Tokens/(1K ticks)/instance = 1680.00\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def print_experiment_metrics(engine):\n",
    "    print(\"# Experiment Config:\")\n",
    "    print(f\"load_generator = {str(engine.load_generator)}\")\n",
    "    print(f\"batcher = {str(engine.batcher)}\")\n",
    "    metrics: sim.Metrics = engine.plot_data.metrics\n",
    "    # we record the latency of every completed request\n",
    "    e2e_latencies = metrics.get_e2e_latencies()\n",
    "    ttfts = metrics.get_ttfts()\n",
    "    itls = metrics.get_itls()\n",
    "\n",
    "    print(\"\\n# Latency Metrics:\")\n",
    "    print(f\"Average E2E Latency: {np.mean(e2e_latencies):.2f}\")\n",
    "    print(f\"Average TTFT: {np.mean(ttfts):.2f}\")\n",
    "    print(f\"Average ITL: {np.mean(itls):.2f}\")\n",
    "    print(f\"Median E2E Latency: {np.percentile(e2e_latencies, 0.5):.2f}\")\n",
    "    print(f\"Median TTFT: {np.percentile(ttfts, 0.5):.2f}\")\n",
    "    print(f\"Median ITL: {np.percentile(itls, 0.5):.2f}\")\n",
    "\n",
    "    print(\"\\n# Throughput Metrics:\")\n",
    "    num_requests: int = len(e2e_latencies)\n",
    "    run_time: float = metrics.times[-1]\n",
    "\n",
    "    # TODO: calculate the throughput in requests/(1000 ticks)/instance\n",
    "    # You have num_requests completed in run_time\n",
    "    # How many complete in 1 tick on average\n",
    "    # How many complete in 1K ticks on average\n",
    "    requests_per_1k_ticks_per_instance: float = -1\n",
    "    \n",
    "    print(f\"Requests/(1K ticks)/instance = {requests_per_1k_ticks_per_instance:.2f}\")\n",
    "\n",
    "    current_batch_tokens = sum(req.tokens_generated for req in engine.current_batch.values())\n",
    "    total_tokens_generated = sum(metrics.get_osls()) + current_batch_tokens\n",
    "    tokens_per_1k_ticks_per_instance = 1000 * total_tokens_generated / run_time\n",
    "    print(f\"Tokens/(1K ticks)/instance = {tokens_per_1k_ticks_per_instance:.2f}\")\n",
    "\n",
    "print_experiment_metrics(engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334c324e-7b79-417c-9762-269f9b0478ef",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Expected Results:</b></summary>\n",
    "\n",
    "```python\n",
    "# Experiment Config:\n",
    "load_generator = BatchLoadGenerator(\n",
    "     prefill_time=2\n",
    "     itl=1\n",
    "     target_output_len_tokens=10\n",
    "     total_prefill_chunks=1\n",
    "     initial_batch=100\n",
    "     target_output_len_std=5\n",
    ")\n",
    "batcher = StaticBatcher\n",
    "\n",
    "# Latency Metrics:\n",
    "Average E2E Latency: 58.16\n",
    "Average TTFT: 52.80\n",
    "Average ITL: 1.00\n",
    "Median E2E Latency: 16.27\n",
    "Median TTFT: 8.00\n",
    "Median ITL: 1.00\n",
    "\n",
    "# Throughput Metrics:\n",
    "Requests/(1K ticks)/instance = 190.00\n",
    "Tokens/(1K ticks)/instance = 1680.00\n",
    "```\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "Note that due to the structure of the load generator, the latency metrics do not make much sense here. You will later learn how to simulate more realistic loads where requests come independently in accordance with some underlying distribution.\n",
    "\n",
    "Since the metrics are very important, we've implemented a function to check your implementation and to print diff between your output, and the expected output. See below.\n",
    "\n",
    "<details>\n",
    "<summary><b>Reveal Solution</b></summary>\n",
    "\n",
    "```python \n",
    "import numpy as np\n",
    "\n",
    "def print_experiment_metrics(engine):\n",
    "    print(\"# Experiment Config:\")\n",
    "    print(f\"load_generator = {str(engine.load_generator)}\")\n",
    "    print(f\"batcher = {str(engine.batcher)}\")\n",
    "    metrics: sim.Metrics = engine.plot_data.metrics\n",
    "    # we record the latency of every completed request\n",
    "    e2e_latencies = metrics.get_e2e_latencies()\n",
    "    ttfts = metrics.get_ttfts()\n",
    "    itls = metrics.get_itls()\n",
    "\n",
    "    print(\"\\n# Latency Metrics:\")\n",
    "    # TODO: calculate np.mean of the latencies list\n",
    "    print(f\"Average E2E Latency: {np.mean(e2e_latencies):.2f}\")\n",
    "    print(f\"Average TTFT: {np.mean(ttfts):.2f}\")\n",
    "    print(f\"Average ITL: {np.mean(itls):.2f}\")\n",
    "\n",
    "    # Otional TODO: calculate median of latencies using np.percentile(array, percentile_value)\n",
    "    print(f\"Median E2E Latency: {np.percentile(e2e_latencies, 0.5):.2f}\")\n",
    "    print(f\"Median TTFT: {np.percentile(ttfts, 0.5):.2f}\")\n",
    "    print(f\"Median ITL: {np.percentile(itls, 0.5):.2f}\")\n",
    "\n",
    "    print(\"\\n# Throughput Metrics:\")\n",
    "    num_requests: int = len(e2e_latencies)\n",
    "    run_time: float = metrics.times[-1]\n",
    "\n",
    "    # TODO: calculate the throughput in requests/(1000 ticks)/instance\n",
    "    # You have num_requests completed in run_time.\n",
    "    # How many complete in 1 tick on average\n",
    "    # How many complete in 1K ticks on average\n",
    "    requests_per_1k_ticks_per_instance: float = -1\n",
    "    requests_per_1k_ticks_per_instance: float = 1000.*num_requests/run_time\n",
    "\n",
    "    print(f\"Requests/(1K ticks)/instance = {requests_per_1k_ticks_per_instance:.2f}\")\n",
    "\n",
    "    current_batch_tokens = sum(req.tokens_generated for req in engine.current_batch.values())\n",
    "    total_tokens_generated = sum(metrics.get_osls()) + current_batch_tokens\n",
    "    tokens_per_1k_ticks_per_instance = 1000 * total_tokens_generated / run_time\n",
    "    print(f\"Tokens/(1K ticks)/instance = {tokens_per_1k_ticks_per_instance:.2f}\")\n",
    "\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04ff9e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Your Implementation\n",
      "\n",
      "+++ Reference\n",
      "\n",
      "@@ -13,11 +13,8 @@\n",
      "\n",
      " Average E2E Latency: 58.16\n",
      " Average TTFT: 52.80\n",
      " Average ITL: 1.00\n",
      "-Median E2E Latency: 16.27\n",
      "-Median TTFT: 8.00\n",
      "-Median ITL: 1.00\n",
      " \n",
      " # Throughput Metrics:\n",
      "-Requests/(1K ticks)/instance = -1.00\n",
      "+Requests/(1K ticks)/instance = 190.00\n",
      " Tokens/(1K ticks)/instance = 1680.00\n",
      " \n"
     ]
    }
   ],
   "source": [
    "from simulator.extra import check_print_metrics\n",
    "\n",
    "# set show median to True if you have implemented the optional TODO\n",
    "check_print_metrics(print_experiment_metrics, engine, show_median=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f114236",
   "metadata": {},
   "source": [
    "<hr><br>\n",
    "\n",
    "## **Inflight Batching (IFB)**\n",
    "\n",
    "IFB is a technique used during LLM inference to balance GPU memory with compute utilization and reduce latency.\n",
    "\n",
    "During auto-regressive inference, the LLM is evaluated from the first layers to the last for every token to generate, using previous tokens to generate the next ones. The process involves:\n",
    "\n",
    "* The first call to the LLM producing the prefill token\n",
    "* Subsequent calls generating the decoding tokens\n",
    "\n",
    "IFB enables sequences at different stages (prefill and decoding) to be processed within the same batch, without requiring all requests to be completed before the next one can enter the batch.\n",
    "\n",
    "**Key Benefits of IFB:**\n",
    "\n",
    "* Allows for a nearly constant batch size for each token, resulting in higher GPU utilization\n",
    "* Enables new request execution to start quicker when slots are available, as the scheduler only needs to wait for the generation of the next token, not the completion of current requests\n",
    "\n",
    "See the illustration below for a visual representation of in-flight batching in TensorRT-LLM:\n",
    "\n",
    "<img src=\"images/ifb_trt-llm.png\" alt=\"In-flight batching in TensorRT-LLM\" width=1200px/>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b74e84",
   "metadata": {},
   "source": [
    "### IFB In Simulation\n",
    "Let's enable IFB in our simulation. First, let's review how `StaticBatcher` works, which cannot batch together prefills and decodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79a5c928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0msim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStaticBatcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_requests\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m <no docstring>\n",
       "\u001b[0;31mSource:\u001b[0m   \n",
       "    \u001b[0;32mdef\u001b[0m \u001b[0madd_requests\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mengine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_occupied_slots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0;31m# static batcher cannot batch together new prefills with old decodings\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mfor\u001b[0m \u001b[0mslot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_all_slots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# checking if we still have more requests to run\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mrequest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign_request_to_slot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m      /dli/task/simulator/batcher.py\n",
       "\u001b[0;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sim.StaticBatcher.add_requests??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa99a5e",
   "metadata": {},
   "source": [
    "As soon as all slots are free, it tries to fill all of them. Now, let's see what changes in the `IFBatcher`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf6ff92e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0msim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIFBatcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_requests\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m <no docstring>\n",
       "\u001b[0;31mSource:\u001b[0m   \n",
       "    \u001b[0;32mdef\u001b[0m \u001b[0madd_requests\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mengine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mempty_slots\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_all_slots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_occupied_slots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mfor\u001b[0m \u001b[0mslot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mempty_slots\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mrequest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign_request_to_slot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m      /dli/task/simulator/batcher.py\n",
       "\u001b[0;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sim.IFBatcher.add_requests??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678d65d9",
   "metadata": {},
   "source": [
    "It fills the slots as soon as they're empty. Let's try it in our previous scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "feb57913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"620\"\n",
       "    src=\"iframe_figures/figure_10.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Experiment Config:\n",
      "load_generator = BatchLoadGenerator(\n",
      "     prefill_time=2\n",
      "     itl=1\n",
      "     target_output_len_tokens=10\n",
      "     total_prefill_chunks=1\n",
      "     initial_batch=100\n",
      "     target_output_len_std=5\n",
      ")\n",
      "batcher = IFBatcher\n",
      "\n",
      "# Latency Metrics:\n",
      "Average E2E Latency: 58.44\n",
      "Average TTFT: 52.90\n",
      "Average ITL: 1.39\n",
      "Median E2E Latency: 16.52\n",
      "Median TTFT: 8.00\n",
      "Median ITL: 1.00\n",
      "\n",
      "# Throughput Metrics:\n",
      "Requests/(1K ticks)/instance = -1.00\n",
      "Tokens/(1K ticks)/instance = 2376.24\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "load_generator = sim.BatchLoadGenerator( \n",
    "        initial_batch=100, # how many requests it sends at engine.current_time == 0\n",
    "        target_output_len_tokens=10,\n",
    "    )\n",
    "load_generator.target_output_len_std = 5\n",
    "\n",
    "engine = sim.Engine(\n",
    "    max_batch_size = 4, # how many slots in the batch\n",
    "    load_generator=load_generator,\n",
    "    batcher = sim.IFBatcher()\n",
    ")\n",
    "engine.run(time_limit=100)\n",
    "engine.plot_data.show()\n",
    "print_experiment_metrics(engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8a5e80",
   "metadata": {},
   "source": [
    "The `Requests/(1K ticks)/instance` are now $267.33$ instead of $190.00$. The effect is more dramatic the bigger the `max_batch_size`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a90280",
   "metadata": {},
   "source": [
    "\n",
    "### Chunked Context\n",
    "To optimize performance, you can separate the prefill into chunks and batch together one chunk of prefill and multiple decodings to attempt a balance between $T_{mem}$ and $T_{math}$. This technique is implemented in TensorRT-LLM as [**Chunked Context**](https://nvidia.github.io/TensorRT-LLM/advanced/gpt-attention.html#chunked-context). It is important to keep chunks large enough to still be able to reach compute-boundness.\n",
    "\n",
    "For our simulation, we assume that our prefills are long enough such that splitting each one into $2$ chunks would still keep us compute-bound. For now, let's keep our batcher as-is to allow for more than $1$ prefill chunk per batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7130ddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"620\"\n",
       "    src=\"iframe_figures/figure_11.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Experiment Config:\n",
      "load_generator = BatchLoadGenerator(\n",
      "     prefill_time=2\n",
      "     itl=1\n",
      "     target_output_len_tokens=10\n",
      "     total_prefill_chunks=2\n",
      "     initial_batch=100\n",
      "     target_output_len_std=5\n",
      ")\n",
      "batcher = IFBatcher\n",
      "\n",
      "# Latency Metrics:\n",
      "Average E2E Latency: 57.42\n",
      "Average TTFT: 54.51\n",
      "Average ITL: 1.14\n",
      "Median E2E Latency: 16.45\n",
      "Median TTFT: 8.00\n",
      "Median ITL: 1.00\n",
      "\n",
      "# Throughput Metrics:\n",
      "Requests/(1K ticks)/instance = -1.00\n",
      "Tokens/(1K ticks)/instance = 2730.00\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "load_generator = sim.BatchLoadGenerator( \n",
    "        initial_batch=100, # how many requests it sends at engine.current_time == 0\n",
    "        target_output_len_tokens=10,\n",
    "        total_prefill_chunks=2, # into how many chuncks can our prefills be split\n",
    "    )\n",
    "load_generator.target_output_len_std = 5\n",
    "\n",
    "engine = sim.Engine(\n",
    "    max_batch_size = 4, # how many slots in the batch\n",
    "    load_generator=load_generator,\n",
    "    batcher = sim.IFBatcher()\n",
    ")\n",
    "engine.run(time_limit=100)\n",
    "engine.plot_data.show()\n",
    "print_experiment_metrics(engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ba0fa9",
   "metadata": {},
   "source": [
    "The `Requests/(1K ticks)/instance` results are now $310.00$ instead of $267.33$, which means we were able to decode more tokens during the same prefill phases using the chunked context strategy!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916ebf1f-c02e-4dad-99a8-39678966678a",
   "metadata": {},
   "source": [
    "### **[EXCERCISE]** Batcher With Only One Prefill Per Batch\n",
    "\n",
    "Given the implementations you saw earlier, complete the `IFBatcherWithOnePrefillOnly` class so that every chunk can contain only one prefill request at a time (i.e. `req.is_in_prefill() == True`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1c4ff6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "class IFBatcherWithOnePrefillOnly(sim.IFBatcher):\n",
    "\n",
    "    def add_requests(self):\n",
    "\n",
    "        engine = self.engine\n",
    "        prefilling_requests: List[sim.Request] = engine.get_prefilling_requests()\n",
    "\n",
    "        ## TODO: check if any requests are already prefilling. If yes, return. \n",
    "        ## See engine.py for engine querying methods.\n",
    "\n",
    "        empty_slots = engine.get_all_slots() - engine.get_occupied_slots()\n",
    "        for slot in empty_slots:\n",
    "            if not len(engine.queue):\n",
    "                break\n",
    "            req = engine.queue.pop(0)\n",
    "            engine.assign_request_to_slot(req, slot)\n",
    "\n",
    "            ## TODO: Make sure no more than one request is added. \n",
    "            ## Otherwise next batch will contain two prefills.\n",
    "\n",
    "## Ground truth implementation accessible below:\n",
    "# IFBatcherWithOnePrefillOnly = sim.IFBatcherWithOnePrefillOnly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9650bc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"620\"\n",
       "    src=\"iframe_figures/figure_13.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Experiment Config:\n",
      "load_generator = BatchLoadGenerator(\n",
      "     prefill_time=2\n",
      "     itl=1\n",
      "     target_output_len_tokens=10\n",
      "     total_prefill_chunks=2\n",
      "     initial_batch=100\n",
      "     target_output_len_std=5\n",
      ")\n",
      "batcher = IFBatcherWithOnePrefillOnly\n",
      "\n",
      "# Latency Metrics:\n",
      "Average E2E Latency: 57.42\n",
      "Average TTFT: 54.51\n",
      "Average ITL: 1.14\n",
      "Median E2E Latency: 16.45\n",
      "Median TTFT: 8.00\n",
      "Median ITL: 1.00\n",
      "\n",
      "# Throughput Metrics:\n",
      "Requests/(1K ticks)/instance = -1.00\n",
      "Tokens/(1K ticks)/instance = 2730.00\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "load_generator = sim.BatchLoadGenerator( \n",
    "    initial_batch=100, # how many requests it sends at engine.current_time == 0\n",
    "    target_output_len_tokens=10,\n",
    "    total_prefill_chunks=2, # this is where we set, into how many chuncks can our prefills be split\n",
    ")\n",
    "\n",
    "load_generator.target_output_len_std = 5\n",
    "\n",
    "engine = sim.Engine(\n",
    "    max_batch_size = 4, # how many slots in the batch\n",
    "    load_generator=load_generator,\n",
    "    batcher = IFBatcherWithOnePrefillOnly()\n",
    ")\n",
    "\n",
    "engine.run(time_limit=100)\n",
    "engine.plot_data.show()\n",
    "print_experiment_metrics(engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0360ab08",
   "metadata": {},
   "source": [
    "If you implemented the class correctly, the `Requests/(1K ticks)/instance` results are now $360.00$ instead of $310.00$! This is the power of balancing $T_{mem}$ and $T_{math}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e642ce20-99da-4428-a643-181da473f708",
   "metadata": {},
   "source": [
    "### Client-Side Concurrency\n",
    "\n",
    "For the IFB engine, the batch size can change after every token, whenever a new request comes or a current one completes. Irregular batch sizes introduce variability when measuring latencies. To achieve more stable latencies, we use client-side concurrency.\n",
    "\n",
    "#### How It Works\n",
    "* If concurrency is set to `C`, the client sends `C` requests simultaneously. \n",
    "* As soon as it gets any request final response,  the client sends another request to maintain the concurrency level.\n",
    "* At any given time, the client has exactly `C` outgoing concurrent requests.\n",
    "\n",
    "Client-side concurrency has been implemented in various tools, including [**Triton Performance Analyzer**](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/client/src/c%2B%2B/perf_analyzer/docs/inference_load_modes.html#concurrency-mode) and in \n",
    "[**GenAI-Perf**](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/perf_analyzer/genai-perf/README.html#concurrency-int). \n",
    "\n",
    "#### How We Simulate It\n",
    "\n",
    "We simulate client-side concurrency using  `sim.ConcurrentLoadGenerator`. Reviewing the source code, we can get a good sense of the load generation strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3418280c-44dc-445f-afdc-e116a6efc06c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0msim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConcurrentLoadGenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m <no docstring>\n",
       "\u001b[0;31mSource:\u001b[0m   \n",
       "    \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mcurrent_concurrency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_occupied_slots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0malready_in_queue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;31m## We want to reach target concurrency but not overshoot it, so limit queue buffer\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mneed_to_add\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_concurrency\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcurrent_concurrency\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0malready_in_queue\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_n_requests_to_queue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneed_to_add\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_generation_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m      /dli/task/simulator/load_generator.py\n",
       "\u001b[0;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sim.ConcurrentLoadGenerator.generate_load??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d614f589-7c7d-4d97-ad78-6819bc0efc4b",
   "metadata": {},
   "source": [
    "Let's replace our load generator with this one and select `target_concurrency=3`. Note that this is less than `max_batch_size`, which should make a noticeable effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b52b5ca9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"620\"\n",
       "    src=\"iframe_figures/figure_15.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Experiment Config:\n",
      "load_generator = ConcurrentLoadGenerator(\n",
      "     prefill_time=2\n",
      "     itl=1\n",
      "     target_output_len_tokens=10\n",
      "     total_prefill_chunks=2\n",
      "     target_concurrency=3\n",
      "     target_output_len_std=5\n",
      ")\n",
      "batcher = IFBatcherWithOnePrefillOnly\n",
      "\n",
      "# Latency Metrics:\n",
      "Average E2E Latency: 11.00\n",
      "Average TTFT: 3.19\n",
      "Average ITL: 1.06\n",
      "\n",
      "# Throughput Metrics:\n",
      "Requests/(1K ticks)/instance = 260.00\n",
      "Tokens/(1K ticks)/instance = 2240.00\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "load_generator = sim.ConcurrentLoadGenerator( # we've updated the generator class\n",
    "    target_concurrency=3, \n",
    "    target_output_len_tokens=10,\n",
    "    total_prefill_chunks=2, # how many chuncks can our prefills be split into\n",
    ")\n",
    "\n",
    "load_generator.target_output_len_std = 5\n",
    "\n",
    "engine = sim.Engine(\n",
    "    max_batch_size = 4, # how many slots in the batch\n",
    "    load_generator=load_generator,\n",
    "    batcher = IFBatcherWithOnePrefillOnly()\n",
    ")\n",
    "\n",
    "engine.run(time_limit=100)\n",
    "engine.plot_data.show()\n",
    "sim.extra.print_experiment_metrics(engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1fa12d",
   "metadata": {},
   "source": [
    "You can see that this is the first example where the queue size and the E2E latency are stationary during the measurements: they do not grow or decline constantly, as you saw in the previous cases. This finally allows us to make sense of the latencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3067b59-d932-4e5e-bce9-dbcc590bd76c",
   "metadata": {},
   "source": [
    "### Max Batch Size\n",
    "\n",
    "TensorRT-LLM engines have two parameters called `max_batch_size`:\n",
    "\n",
    "- One is set for the engine build and is used during the kernel selection process to make sure the resulting batch-size-capable system fits into memory.\n",
    "- One is set for runtime and specifies how many requests can be batched together. This is the one we use in our simulation.\n",
    "\n",
    "Note that the second one should be less than or equal to the first one. See the [docs](https://nvidia.github.io/TensorRT-LLM/performance/perf-best-practices.html#max-batch-size) for details.\n",
    "\n",
    "#### Batch Size and Concurrency\n",
    "\n",
    "Let's assume we've set a runtime `max_batch_size` to value $MBS$ and we're running our benchmark with concurrency $C$. If $C < MBS$ as we set before, the engine has free slots available in the batches and is usually running with batch size $C$. If $C >= MBS$, then the engine is usually running with no free slots and the batch size for most of the batches is $MBS$. The queue size is then $C - MBS$ on average.\n",
    "\n",
    "Note that the real client doesn’t have to care about available slots in the server: the batch is almost static anyway. For details on how this happens, see appendix [below](#On-concurrency-and-request-rate-as-an-input-parameter).\n",
    "\n",
    "Let's have a look at the example, where $ C = 6 > MBS = 4$\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76523150",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"620\"\n",
       "    src=\"iframe_figures/figure_16.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Experiment Config:\n",
      "load_generator = ConcurrentLoadGenerator(\n",
      "     prefill_time=2\n",
      "     itl=1\n",
      "     target_output_len_tokens=10\n",
      "     total_prefill_chunks=2\n",
      "     target_concurrency=6\n",
      "     target_output_len_std=5\n",
      ")\n",
      "batcher = IFBatcherWithOnePrefillOnly\n",
      "\n",
      "# Latency Metrics:\n",
      "Average E2E Latency: 15.14\n",
      "Average TTFT: 7.87\n",
      "Average ITL: 1.00\n",
      "\n",
      "# Throughput Metrics:\n",
      "Requests/(1K ticks)/instance = 360.00\n",
      "Tokens/(1K ticks)/instance = 3170.00\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "load_generator = sim.ConcurrentLoadGenerator( \n",
    "        target_concurrency=6, # increased from last time\n",
    "        target_output_len_tokens=10,\n",
    "        total_prefill_chunks=2, # how many chuncks can our prefills be split into\n",
    "    )\n",
    "load_generator.target_output_len_std = 5\n",
    "\n",
    "engine = sim.Engine(\n",
    "    max_batch_size = 4, # how many slots in the batch\n",
    "    load_generator=load_generator,\n",
    "    batcher = sim.IFBatcherWithOnePrefillOnly()\n",
    ")\n",
    "engine.run(time_limit=100)\n",
    "engine.plot_data.show()\n",
    "sim.extra.print_experiment_metrics(engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21b3a37",
   "metadata": {},
   "source": [
    "You may recall we got the same maximum throughput of $360$ from our static batching exercise, but we can now measure the TTFT and E2E latency at which such throughput can be achieved.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb4958f",
   "metadata": {},
   "source": [
    "\n",
    "<br><hr>\n",
    "\n",
    "## **Additional Notes**\n",
    "\n",
    "---\n",
    "\n",
    "### On concurrency and request rate as a result metric\n",
    "\n",
    "To effectively measure system performance, it's essential to consider throughput, end-to-end latency, and concurrency. A hypothetical server capable of handling 60 simultaneous requests with an e2e latency of 20 seconds each, achieves a throughput of 3 requests per second. This throughput reflects the system’s ability to process multiple requests concurrently, offsetting the high latency of individual requests.\n",
    "\n",
    "Comparatively, consider two systems: one with a batch size of 60 and a 20-second latency, and another with a batch size of 30 and a 10-second latency. Both process 3 requests per second on average, but the latter provides faster responses, demonstrating superior efficiency despite its lower concurrency.\n",
    "\n",
    "Thus, we recommend using requests per minute as the primary metric for system sizing and communication with stakeholders. This metric ensures a clear and balanced understanding of system capacity and creates an opportunity to factor in concurrency/latency requirements.\n",
    "\n",
    "```python\n",
    "requests_per_second = concurrent_users * requests_per_session / session_duration_in_seconds\n",
    "requests_per_minute = 60 * requests_per_second\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### On concurrency and request rate as an input parameter\n",
    "\n",
    "For good speed measurements, we need engine batch size to be constant from token to token. Below we explain why using concurrency as an input for speed measurements helps us achieve it, and why one should avoid using the request rate as an input parameter.\n",
    "\n",
    "Let's simulate some moderate request rate of one request in 5 ticks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2b9a685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"620\"\n",
       "    src=\"iframe_figures/figure_17.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Experiment Config:\n",
      "load_generator = RequestRateLoadGenerator(\n",
      "     prefill_time=2\n",
      "     itl=1\n",
      "     target_output_len_tokens=10\n",
      "     total_prefill_chunks=2\n",
      "     request_rate=0.2\n",
      "     target_output_len_std=5\n",
      ")\n",
      "batcher = IFBatcherWithOnePrefillOnly\n",
      "\n",
      "# Latency Metrics:\n",
      "Average E2E Latency: 10.06\n",
      "Average TTFT: 2.00\n",
      "Average ITL: 1.00\n",
      "\n",
      "# Throughput Metrics:\n",
      "Requests/(1K ticks)/instance = 170.00\n",
      "Tokens/(1K ticks)/instance = 1650.00\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "load_generator = sim.RequestRateLoadGenerator( \n",
    "        request_rate=1./5., # 1 request in 5 ticks\n",
    "        target_output_len_tokens=10,\n",
    "        total_prefill_chunks=2, # this is where we set, into how many chuncks can our prefills be split\n",
    "    )\n",
    "load_generator.target_output_len_std = 5\n",
    "\n",
    "engine = sim.Engine(\n",
    "    max_batch_size = 4, # how many slots in the batch\n",
    "    load_generator=load_generator,\n",
    "    batcher = sim.IFBatcherWithOnePrefillOnly()\n",
    ")\n",
    "engine.run(time_limit=100)\n",
    "engine.plot_data.show()\n",
    "sim.extra.print_experiment_metrics(engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4f6763",
   "metadata": {},
   "source": [
    "In this case, the queue is 0 all the time, and our latency measurements can be similar to reality. Note two things about these measurements:\n",
    "1. A much longer warmup period is observed when the engine is running with a low batch size.\n",
    "2. The resulting throughput difference, where $1000 * 1 / 5 = 200$ requests/1k ticks is expected but $170$ is observed.\n",
    "\n",
    "Now, let's consider what happens when our request rate exceeds the capability of our engine by trying a rate of $460$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab8ded54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"620\"\n",
       "    src=\"iframe_figures/figure_18.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Queue: 6\n",
      "# Experiment Config:\n",
      "load_generator = RequestRateLoadGenerator(\n",
      "     prefill_time=2\n",
      "     itl=1\n",
      "     target_output_len_tokens=10\n",
      "     total_prefill_chunks=2\n",
      "     request_rate=0.46\n",
      "     target_output_len_std=5\n",
      ")\n",
      "batcher = IFBatcherWithOnePrefillOnly\n",
      "\n",
      "# Latency Metrics:\n",
      "Average E2E Latency: 17.66\n",
      "Average TTFT: 11.03\n",
      "Average ITL: 1.00\n",
      "\n",
      "# Throughput Metrics:\n",
      "Requests/(1K ticks)/instance = 350.00\n",
      "Tokens/(1K ticks)/instance = 3060.00\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "load_generator = sim.RequestRateLoadGenerator( \n",
    "        request_rate=460./1000., # 460 requests in 1000 ticks\n",
    "        target_output_len_tokens=10,\n",
    "        total_prefill_chunks=2, # this is where we set, into how many chuncks can our prefills be split\n",
    "    )\n",
    "load_generator.target_output_len_std = 5\n",
    "\n",
    "engine = sim.Engine(\n",
    "    max_batch_size = 4, # how many slots in the batch\n",
    "    load_generator=load_generator,\n",
    "    batcher = sim.IFBatcherWithOnePrefillOnly()\n",
    ")\n",
    "engine.run(time_limit=100)\n",
    "engine.plot_data.show()\n",
    "print(f\"Final Queue: {len(engine.queue)}\")\n",
    "sim.extra.print_experiment_metrics(engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6392ec",
   "metadata": {},
   "source": [
    "This is the key caveat of using the request rate. In this situation, there are no free slots available. While the throughput is pushed to the system's limit, the client keeps sending more and more requests and the queue grows with the approximate speed of $460 - 360 = 100$ requests per 1k ticks. The final average TTFT depends on the duration of the measurement and final queue size, but it is impossible to identify the actual latency of the requests at that point. That's why we recommend avoiding request rate as a measurement input. In a real-world scenario, the system tries to autoscale if it reaches high enough latencies and has the resources to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb5c8d9",
   "metadata": {},
   "source": [
    "**Overall we saw 5 use cases represented:**\n",
    "\n",
    "1. **Static batching with MBS:** The prefills are batched together and don't benefit from the overlap of computations and memory-intensive operations. The E2E and TTFT latency is high. In reality, in static batching the requests wait on average E2E Latency$/ 2$ in the queue. This adds to both real TTFT and E2E Latency. That's why nowadays static batching is almost universally deprecated for LLM inference.\n",
    "2. **Concurrency < MBS:** There are almost no free slots available and no queue is forming. The engine is running with a maximum throughput of 280 requests/1000 ticks. TTFT is 2.67 and E2E Latency is 10.14.\n",
    "3. **Concurrency > MBS:** There are almost no free slots available and a queue of size 2 is forming. The engine is running with a maximum throughput of $360$ requests/100 ticks. However, TTFT is growing with the queue size. It is now 7.87 because every request has to wait until the slots are available. E2E Latency grows with the TTFT and is now 15.14\n",
    "4. **Request Rate < Maximum Throughput:** In this situation there are free slots available. The situation is almost stationary and batch size is almost constant and equal to 3. The throughput is closely defined by the request rate we set. This is a valid measurement.\n",
    "5. **Request Rate > Maximum Throughput:** Correct maximum throughput, but invalid measurements. See the details above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f937bd89",
   "metadata": {},
   "source": [
    "### **[OPTIONAL EXCERCISE]** Queue Growth\n",
    "\n",
    "Create longer simulations (10000 ticks) for the same setups with `RequestRateLoadGenerator` and with `ConcurrentLoadGenerator`. \n",
    "* Avoid plotting them, to keep the memory consumption reasonable (comment out `engine.plot_data.show()`)\n",
    "* Check the queue size at the end of the execution using `len(engine.queue)`\n",
    "* Compare TTFT with 100 ticks as the time limit as above with 10000 ticks in both cases\n",
    "* Compare E2E Latency with 100 ticks as the time limit as above with 10000 ticks in both cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "259d6cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Create batcher\n",
    "# TODO Create ConcurrentLoadGenerator\n",
    "# TODO Create engine\n",
    "# TODO Run engine to 100 ticks and print metrics\n",
    "# TODO Run engine to 10000 ticks and print metrics\n",
    "# TODO Create ConcurrentLoadGenerator\n",
    "# TODO Create another engine\n",
    "# TODO Run engine to 100 ticks and print metrics\n",
    "# TODO Run engine to 10000 ticks and print metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d81479",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
