{
  "request_throughput": {
    "unit": "requests/sec",
    "avg": 2.9266732257910606
  },
  "request_latency": {
    "unit": "ms",
    "avg": 683.2680207211538,
    "p99": 690.25105262,
    "p95": 689.10564455,
    "p90": 688.2444489,
    "p75": 683.58681775,
    "p50": 682.2828109999999,
    "p25": 681.66332825,
    "max": 690.311515,
    "min": 680.339487,
    "std": 2.5979987042654673
  },
  "time_to_first_token": {
    "unit": "ms",
    "avg": 34.702941682692305,
    "p99": 40.751330939999995,
    "p95": 40.57352495,
    "p90": 40.177095,
    "p75": 34.102467749999995,
    "p50": 33.7485535,
    "p25": 33.43315425,
    "max": 40.770210999999996,
    "min": 32.888602999999996,
    "std": 2.4520101360546014
  },
  "inter_token_latency": {
    "unit": "ms",
    "avg": 11.161048826923075,
    "p99": 12.661941409999999,
    "p95": 12.198480199999999,
    "p90": 12.0047969,
    "p75": 11.59065275,
    "p50": 11.1702725,
    "p25": 10.795326,
    "max": 12.698868,
    "min": 9.684963999999999,
    "std": 0.6493281019651042
  },
  "output_token_throughput": {
    "unit": "tokens/sec",
    "avg": 173.5742351603775
  },
  "output_token_throughput_per_request": {
    "unit": "tokens/sec",
    "avg": 86.80121863494777,
    "p99": 98.03707435680771,
    "p95": 96.43115792777829,
    "p90": 93.31306025308342,
    "p75": 89.51708919539736,
    "p50": 86.50937077550645,
    "p25": 83.51320827255554,
    "max": 99.68280316345486,
    "min": 76.28565956835793,
    "std": 5.007161231705726
  },
  "output_sequence_length": {
    "unit": "tokens",
    "avg": 59.30769230769231,
    "p99": 67.0,
    "p95": 65.85,
    "p90": 64.0,
    "p75": 61.0,
    "p50": 59.0,
    "p25": 57.0,
    "max": 68.0,
    "min": 52.0,
    "std": 3.4139896666198304
  },
  "input_sequence_length": {
    "unit": "tokens",
    "avg": 200.02884615384616,
    "p99": 200.97,
    "p95": 200.0,
    "p90": 200.0,
    "p75": 200.0,
    "p50": 200.0,
    "p25": 200.0,
    "max": 202.0,
    "min": 200.0,
    "std": 0.21735874145110223
  },
  "input_config": {
    "model": [
      "meta/llama3-8b-instruct"
    ],
    "model_selection_strategy": "round_robin",
    "backend": "tensorrtllm",
    "endpoint": "v1/chat/completions",
    "endpoint_type": "chat",
    "service_kind": "openai",
    "streaming": true,
    "u": "nim:8000",
    "batch_size": 1,
    "input_dataset": null,
    "num_prompts": 100,
    "output_tokens_mean": 50,
    "output_tokens_mean_deterministic": false,
    "output_tokens_stddev": 0,
    "random_seed": 0,
    "synthetic_input_tokens_mean": 200,
    "synthetic_input_tokens_stddev": 0,
    "concurrency": 2,
    "measurement_interval": 10000,
    "request_rate": null,
    "stability_percentage": 999,
    "artifact_dir": "artifacts/meta_llama3-8b-instruct-openai-chat-concurrency2",
    "generate_plots": false,
    "profile_export_file": "artifacts/meta_llama3-8b-instruct-openai-chat-concurrency2/200_50.json",
    "tokenizer": "hf-internal-testing/llama-tokenizer",
    "verbose": false,
    "subcommand": null,
    "prompt_source": "synthetic",
    "formatted_model_name": "meta/llama3-8b-instruct",
    "extra_inputs": {
      "max_tokens": 50,
      "min_tokens": 50,
      "ignore_eos": true
    }
  }
}