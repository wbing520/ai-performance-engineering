{
  "request_throughput": {
    "unit": "requests/sec",
    "avg": 6.605343676290772
  },
  "request_latency": {
    "unit": "ms",
    "avg": 756.7700712680851,
    "p99": 768.80203222,
    "p95": 760.9374453999999,
    "p90": 758.6957436,
    "p75": 757.4542299999999,
    "p50": 756.184666,
    "p25": 755.395058,
    "max": 768.819046,
    "min": 753.343526,
    "std": 2.448509344067461
  },
  "time_to_first_token": {
    "unit": "ms",
    "avg": 78.16427731489361,
    "p99": 87.10824777999999,
    "p95": 79.38449659999999,
    "p90": 79.0912242,
    "p75": 78.831318,
    "p50": 78.480182,
    "p25": 78.2456555,
    "max": 88.13013099999999,
    "min": 26.074493,
    "std": 5.272447027209628
  },
  "inter_token_latency": {
    "unit": "ms",
    "avg": 11.867379629787234,
    "p99": 13.56098318,
    "p95": 13.0076556,
    "p90": 12.776155199999998,
    "p75": 12.322725,
    "p50": 11.883702999999999,
    "p25": 11.3369045,
    "max": 13.908655,
    "min": 9.971517,
    "std": 0.7010576995030597
  },
  "output_token_throughput": {
    "unit": "tokens/sec",
    "avg": 385.61153146822596
  },
  "output_token_throughput_per_request": {
    "unit": "tokens/sec",
    "avg": 77.14292293242065,
    "p99": 88.71809771496132,
    "p95": 84.73054495768403,
    "p90": 83.11286070916839,
    "p75": 80.37924161877424,
    "p50": 76.7387233355677,
    "p25": 74.02327984667457,
    "max": 90.45789704894182,
    "min": 66.20191807982407,
    "std": 4.4704730404809485
  },
  "output_sequence_length": {
    "unit": "tokens",
    "avg": 58.37872340425532,
    "p99": 67.0,
    "p95": 64.0,
    "p90": 63.0,
    "p75": 61.0,
    "p50": 58.0,
    "p25": 56.0,
    "max": 69.0,
    "min": 50.0,
    "std": 3.37657362338408
  },
  "input_sequence_length": {
    "unit": "tokens",
    "avg": 200.02978723404254,
    "p99": 201.0,
    "p95": 200.0,
    "p90": 200.0,
    "p75": 200.0,
    "p50": 200.0,
    "p25": 200.0,
    "max": 202.0,
    "min": 200.0,
    "std": 0.21429239680024126
  },
  "input_config": {
    "model": [
      "meta/llama3-8b-instruct"
    ],
    "model_selection_strategy": "round_robin",
    "backend": "tensorrtllm",
    "endpoint": "v1/chat/completions",
    "endpoint_type": "chat",
    "service_kind": "openai",
    "streaming": true,
    "u": "nim:8000",
    "batch_size": 1,
    "input_dataset": null,
    "num_prompts": 100,
    "output_tokens_mean": 50,
    "output_tokens_mean_deterministic": false,
    "output_tokens_stddev": 0,
    "random_seed": 0,
    "synthetic_input_tokens_mean": 200,
    "synthetic_input_tokens_stddev": 0,
    "concurrency": 5,
    "measurement_interval": 10000,
    "request_rate": null,
    "stability_percentage": 999,
    "artifact_dir": "artifacts/meta_llama3-8b-instruct-openai-chat-concurrency5",
    "generate_plots": false,
    "profile_export_file": "artifacts/meta_llama3-8b-instruct-openai-chat-concurrency5/200_50.json",
    "tokenizer": "hf-internal-testing/llama-tokenizer",
    "verbose": false,
    "subcommand": null,
    "prompt_source": "synthetic",
    "formatted_model_name": "meta/llama3-8b-instruct",
    "extra_inputs": {
      "max_tokens": 50,
      "min_tokens": 50,
      "ignore_eos": true
    }
  }
}