version: '2.3'

## To Build The Repo: Execute the following command from directory ABOVE notebook repository
## Dev Environment:  docker-compose build && docker-compose up -d
## Prod Environment: docker-compose -f docker-compose.yml -f docker-compose.deployment.yml build && ... 

volumes:
  ## Create a shared volume to store and move around your model information
  model-store:
    # driver: local
    # driver_opts:
    #   o: bind
    #   device: ${PWD}/model-store
    #   type: none

services:  

  ## Deliver jupyter labs interface for students.
  lab:
    init: true
    privileged: true
    runtime: nvidia
    shm_size: 128gb
    ulimits:
      memlock: -1
      stack: 67108864
    volumes:
      - ./notebooks/:/dli/task/
      # - model-store:/dli/task/model-store
      ## Below lines give access to host docker.
      # - /var/run/docker.sock:/var/run/docker.sock
      # - /var/lib/docker/:/var/lib/docker/
    environment:
      - HF_HUB_ENABLE_HF_TRANSFER=TRUE

  #############################################################################
  ## Other Stuff: DLI-Specific

  ## Reverse-proxy (serving the front page) service
  nginx:
    image: nginx:1.15.12-alpine
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
    depends_on:
      - lab

  nim:
    user: root # you don't have to be the root, but need to specify some user
    container_name: nim 
    image: nvcr.io/nim/meta/llama3-8b-instruct:1.0.3
    runtime: nvidia
    shm_size: 16gb
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      # Necessary since we are running as root on potentially-multiple GPUs
      - OMPI_ALLOW_RUN_AS_ROOT=1
      - OMPI_ALLOW_RUN_AS_ROOT_CONFIRM=1
      - end_id
    volumes:
      - ./nim-cache/nim:/opt/nim/.cache
    env_file:
      - .env  ## pass in your environment variable through file (i.e. NGC_API_KEY)

  ## Deliver a server to interface with host docker orchestration
  docker_router:
    container_name: docker_router
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock  ## Give access to host docker
    ports:
      - "8070:8070"


networks:
  default:
    name: nvidia-sizing
