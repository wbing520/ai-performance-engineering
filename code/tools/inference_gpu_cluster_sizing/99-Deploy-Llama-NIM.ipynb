{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c999b685",
   "metadata": {},
   "source": [
    "# <font color=\"#76b900\">**Notebook 99:** Deploying Your First NIM</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ce7e6e",
   "metadata": {},
   "source": [
    "In this notebook, we will be loading in a NIM model to run on your GPU-enabled environment. The process is documented extensively in the [NIM \"Getting Started\" documentation](https://docs.nvidia.com/nim/large-language-models/latest/getting-started.html), so feel free to refer to it if you need any more details. \n",
    "\n",
    "<hr>\n",
    "\n",
    "### **NOTE:** THIS IS ONLY FOR REFERENCE!\n",
    "**(It also will not work in this environment since it assumes host docker access)**\n",
    "\n",
    "<hr>\n",
    "\n",
    "NIMs are packaged as container images on a per model/model family basis. These containers include a runtime that runs on any NVIDIA GPU with sufficient GPU memory, but some model/GPU combinations like this one are especially well-optimized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8743b620-1a16-4bd5-9102-3bb2e7f7ecc9",
   "metadata": {},
   "source": [
    "### **Kickstarting On Application Startup**\n",
    "\n",
    "One way to kickstart the environment alongside other microservices is to specify it in the docker-compose file as shown below and demonstrated in [`composer/docker-compose.yml`](./composer/docker-compose.yml):\n",
    "\n",
    "```sh\n",
    "  nim:\n",
    "    ## VLLM-backed LLM NIM: Provides speedup while also being easy to work with\n",
    "    image: nvcr.io/nim/meta/llama3-8b-instruct:1.0.3  ## <-the image you're working with\n",
    "    entrypoint: /bin/sh -c \"echo 'nim image downloaded'\"\n",
    "    deploy:\n",
    "      resources:\n",
    "        reservations:\n",
    "          devices:\n",
    "            - driver: nvidia\n",
    "              count: 1\n",
    "              capabilities: [gpu]\n",
    "    ports:\n",
    "      - 8000:8000\n",
    "    entrypoint: env\n",
    "    environment:\n",
    "      NGC_API_KEY: ${NGC_API_KEY}  ## <- needs to be specified here or in env file\n",
    "```\n",
    "\n",
    "The one catch is that we have yet to provide the NGC_API_KEY, which is used internally inside the NIM image to help download the right model. To kickstart the microservice, please sign up for an NGC account if you do not already have one. Once there, navigate to [**`Setup Menu`**](https://org.ngc.nvidia.com/setup) and generate your API key in **`Generate API Key`**. This should give an API key which is used to connect to the NGC model registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fe7f95-dc4b-4cc8-abb5-3dce2ee4e0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Add your NGC API KEY ()\n",
    "## NOTE: It should NOT start with `nvapi-`. That's for NVCF & build.nvidia.com endpoints.\n",
    "%env NGC_API_KEY=..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f87e1aa-85f8-4475-8bef-a9b374485892",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo \"$NGC_API_KEY\" | docker login nvcr.io --username \"\\$oauthtoken\" --password-stdin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2811ee21",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "After this, you should be able to select your models of choice from the [**NGC Catalog**](https://catalog.ngc.nvidia.com/containers?filters=nvidia_nim), though in this course we will be defaulting to [**Llama-3-8B**](https://catalog.ngc.nvidia.com/orgs/nim/teams/meta/containers/llama3-8b-instruct). The code block below specifies the model of interest and then proceeds to actually download the NIM, download the associated model, and kickstart the microservice with docker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6986dff8-c19e-4932-a5ec-a5b9c0d9cc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a LLM NIM Image from NGC\n",
    "%env IMG_NAME=nvcr.io/nim/meta/llama3-8b-instruct:1.0.3\n",
    "\n",
    "# Choose a path on your system to cache the downloaded models\n",
    "%env LOCAL_NIM_CACHE=./cache/nim\n",
    "\n",
    "!mkdir -p \"$LOCAL_NIM_CACHE\"\n",
    "\n",
    "## Start the LLM NIM\n",
    "# !docker rm -f nim\n",
    "!docker run --rm --name=nim \\\n",
    "    --runtime=nvidia \\\n",
    "    --gpus all \\\n",
    "    --shm-size=16GB \\\n",
    "    --network nvidia-sizing \\\n",
    "    -e NGC_API_KEY=$NGC_API_KEY \\\n",
    "    -v $LOCAL_NIM_CACHE:/opt/nim/.cache \\\n",
    "    -u $(id -u) \\\n",
    "    -p 8000:8000 \\\n",
    "    $IMG_NAME\n",
    "\n",
    "## NOTES:\n",
    "## - gpus, runtime, and shared-memory-size all help specify which resources \n",
    "##     and environments the service can leverage. \n",
    "## - Since we put our microservices (including our jupyter labs instance)\n",
    "##     on the nvidia-sizing network in the dockerfile, specifying this helps\n",
    "##     the services play nicely together. This detail is unimportant.\n",
    "## - -e specifies environment variables, and NIMs want access to NGC.\n",
    "## - -v specifies volume mounts, and we may want access to the NIM model cache\n",
    "##     to avoid redownloading resources and maybe modify some settings.\n",
    "## - -u specifies user profile and -p specifies port mapping. We usually \n",
    "##     want parity with our other services to keep things consistent.\n",
    "## - $IMG_NAME AKA nvcr.io/nim/<model-name> here is the image we run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11a15df-d85e-4e07-850f-5f112f920ae4",
   "metadata": {},
   "source": [
    "<br><hr>\n",
    "\n",
    "#### **WHEN YOU SEE THE FOLLOWING, YOU CAN ASSUME THE SERVICE IS RUNNING:**\n",
    "```json\n",
    "INFO ... on.py:62] Application startup complete.\n",
    "INFO ... server.py:214] Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n",
    "INFO...8 metrics.py:334] Avg prompt throughput: 0.3 tokens/s, Avg generation throughput: 1.5 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
