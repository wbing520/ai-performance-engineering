# Chapter 17: Scaling Disaggregated Prefill and Decode for Inference

## Summary
These examples demonstrate disaggregated inference control planes—dynamic routing, QoS tiers, and early rejection—to meet SLOs under variable load.

## Performance Takeaways
- Route requests dynamically based on load, cache hits, and SLO pressures
- Protect SLOs with early rejection and multi‑tier QoS policies
- Apply latency‑aware scheduling heuristics and measure impact
- Autoscale prefill/decode pools to maintain efficiency under bursts
- Increase goodput on mixed traffic while tightening tail latency

Code examples demonstrating disaggregated inference systems with dynamic routing and quality of service controls.

## Examples

- `dynamic_routing.py` - Dynamic routing algorithms for prefill/decode disaggregation
- `early_rejection.py` - Early rejection and QoS policies for ultra-scale inference
- `dynamo_config.yaml` - Sample NVIDIA Dynamo configuration (generated by examples)

## Key Concepts

- **Disaggregated Architecture**: Separating prefill and decode phases onto specialized hardware
- **Dynamic Routing**: Intelligent request routing based on load, cache hits, and SLOs
- **Early Rejection**: Admission control to prevent system overload
- **Quality of Service**: Multi-tier service levels with reserved capacity
- **Latency-Aware Scheduling**: Cost-based worker selection for optimal performance

## Requirements

- PyTorch 2.8+
- PyYAML for configuration files
- Threading support for concurrent simulation

## Usage

### Dynamic Routing Demo
```bash
# Run the dynamic routing simulation
python dynamic_routing.py

# This will create dynamo_config.yaml and demonstrate:
# - Conditional prefill offloading
# - Latency-aware worker selection  
# - Load balancing across worker pools
```

### Early Rejection and QoS Demo
```bash
# Run the QoS simulation with load spikes
python early_rejection.py

# Demonstrates:
# - Multi-tier priority systems
# - SLO-aware admission control
# - Graceful degradation under load
```

## Configuration

The examples support YAML configuration files following NVIDIA Dynamo's format:

```yaml
model: "llama-70b"
split_policy:
  prompt_length_threshold: 256
  prefix_cache_weight: 10.0
  queue_length_weight: 1.5
  enable_hotspot_prevention: true
autoscale:
  prefill:
    min_replicas: 4
    max_replicas: 12
  decode:
    min_replicas: 8
    max_replicas: 24
qos:
  enable_early_rejection: true
  reject_on_slo_violation: true
```

## Key Algorithms

### Prefill Offloading Decision
From Chapter 17's routing logic:
```python
def should_offload_prefill(prompt_length, prefix_cached_length, prefill_queue_size):
    # Long effective prefill AND prefill workers available
    long_prefill = (prompt_length - prefix_cached_length) > THRESHOLD
    prefill_available = prefill_queue_size < QUEUE_MAX
    return long_prefill and prefill_available
```

### Early Rejection Policy
```python
def admit_request(priority):
    est_ttft = queue_length * avg_time_per_request
    if est_ttft > SLO_MAX:
        return priority != "low"  # Reject low priority
    return True
```

## Performance Benefits

Disaggregated inference provides:
- **7.4× higher goodput** (DistServe results) within latency SLOs
- **Tighter latency distributions** by eliminating cross-phase interference
- **Phase-specific optimizations** with specialized hardware
- **Better resource utilization** through intelligent routing
