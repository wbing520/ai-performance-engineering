{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7eb82c3b",
   "metadata": {},
   "source": [
    "# <font color=\"#76b900\">**Notebook 2:** Benchmarking Throughput/Latency Tradeoffs</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d791d79",
   "metadata": {},
   "source": [
    "Welcome back to the notebooks! In this course, we will be heavily focusing on the speed of an LLM to produce a response from the user's perspective. The process of measuring this speed is called **benchmarking**, which you will get the chance to do in the subsequent notebooks using a tool called [**GenAI-Perf**](https://github.com/triton-inference-server/client/tree/main/src/c%2B%2B/perf_analyzer/genai-perf). To get started, a set of pre-collected benchmarks is provided in [**dataset/nim.csv**](dataset/nim.csv) for you to explore throughout this notebook!\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this notebook, you will be able to:\n",
    "- Explore the real measurements of the time to first token (TTFT), end-to-end latency (E2E Latency), and inter-token latency (ITL).\n",
    "- Analyze throughput metrics and their dependencies on various factors.\n",
    "- Investigate the effects of tensor parallelism and concurrency settings on latency and throughput.\n",
    "- Utilize provided benchmarking data to make informed decisions about model deployment and scaling.\n",
    "\n",
    "**Before starting this notebook, please make sure to watch its corresponding video.**\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "- [**Getting Started With The Dataset**](#Getting-Started-With-The-Dataset)\n",
    "  - [Use-Case Definition Columns](#Use-Case-Definition-Columns)\n",
    "  - [Benchmarking Results](#Benchmarking-Results)\n",
    "  - [Investigating Benchmarked Data](#Investigating-Benchmarked-Data)\n",
    "  - [**Plotting Latency vs Throughput**](#Plotting-Latency-vs-Throughput)\n",
    "  - [Latency vs Throughput Tradeoff](#Latency-vs-Throughput-Tradeoff)\n",
    "- [**Tensor Parallelism**](#Tensor-Parallelism)\n",
    "  - [Benefits of Tensor Parallelism N](#Benefits-of-Tensor-Parallelism-N)\n",
    "  - [**Scenario**: 1xTP2 vs 2xTP1](#Scenario:-1xTP2-vs-2xTP1)\n",
    "  - [**Scenario**: Fluctuating Tokens-Per-Second](#Scenario:-Fluctuating-Tokens-Per-Second)\n",
    "  - [**Scenario**: TTFT Incorporated Into Request](#Scenario:-TTFT-Incorporated-Into-Request)\n",
    "  - [**Scenario**: FP8 vs FP16](#Scenario:-FP8-vs-FP16)\n",
    "- [**[EXERCISE] A Use-Case-Based Sizing Example**](#[EXERCISE]-A-Use-Case-Based-Sizing-Example)\n",
    "- [**Sizing Best Practices**](#Sizing-Best-Practices)\n",
    "  - [NVIDIA Software Stack](#NVIDIA-Software-Stack)\n",
    "  - [Model Size and Hardware Considerations](#Model-Size-and-Hardware-Considerations)\n",
    "  - [Streaming vs. Sequential Mode](#Streaming-vs.-Sequential-Mode)\n",
    "  - [Other Performance Considerations](#Other-Performance-Considerations)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355e9fb2",
   "metadata": {},
   "source": [
    "<br><hr>\n",
    "\n",
    "## **Getting Started With The Dataset**\n",
    "\n",
    "The provided [**dataset/nim.csv datasheet**](dataset/nim.csv) is a simple csv file with each row corresponding to a benchmark run against an older version of NVIDIA NIM. Most of the columns are described below. Some of the names have a shortened form used in code to tidy up the processing, so below we indicate two names for many columns. For the details refer to the  [**GenAI-Perf documentation on metrics**](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/client/src/c%2B%2B/perf_analyzer/genai-perf/README.html#metrics) and [**its source code**](https://github.com/triton-inference-server/client/blob/db888f1aca588a10f5e4a4b02a4e4ff60d437b6f/src/c%2B%2B/perf_analyzer/genai-perf/genai_perf/profile_data_parser/llm_profile_data_parser.py#L76).\n",
    "\n",
    "Below are useful definitions to help interpret the datasheet. Feel free to [skip ahead for now](#skip-definitions) and come back when you need them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf9b90e-09a8-4f66-bc59-9b0cb2999946",
   "metadata": {},
   "source": [
    "### Use-Case Definition Columns\n",
    "* `'task_inputs-model'`, `'model'`: The LLM being tested.\n",
    "* `'task_inputs-execution_type'`, `'execution_mode'`: We will be testing only `NIM_TRT-LLM`.\n",
    "* `'task_inputs-precision'`, `'precision'`: In our case, `FP16` or `FP8`. Due to the HW acceleration supported in Hopper architecture, one may significantly benefit from reducing the precision, while keeping the accuracy close to the original.\n",
    "* `'input_config-synthetic_input_tokens_mean'`, `input_len`: We are using a synthetic dataset generator, built into GenAI-Perf. This is the target mean of the distribution of the LLM inputs in tokens.\n",
    "* `'input_config-synthetic_input_tokens_stddev'`: This is the target standard deviation of the input lengths in tokens.\n",
    "* `'input_config-output_tokens_mean'`, `output_len`: For benchmarking, we force the LLM to generate a specific number of tokens and ignore End Of Sequence (EOS) tokens. This is a simple trick to simulate the responses, similar in length to the real ones, even without using a real data subsample. Real datasets are also supported by GenAI-Perf.\n",
    "* `'task_inputs-n_gpus'`, `n_gpus`: Number of GPUs. In our case, the model is parallelized across GPUs using tensor parallelism only (TP). See [the *Tensor Parallelism* section](#Tensor-Parallelism) for details.\n",
    "* `'task_inputs-GPU'`, `'device'`: The type of the GPU the benchmark has been using. In our case, the benchmark has been run on a DGX A100 with `A100-SXM4-80GB` and on a DGX H100 with `H100_80GB_HBM3`.\n",
    "* `'input_config-concurrency'`, `'concurrency'`: Concurrency. With `concurrency=N`, GenAI-Perf attempts to send inference requests to the server such that N requests are always outstanding during profiling. For example, when using 4, GenAI-Perf will attempt to have 4 outgoing inference requests at all times during profiling. At the start of benchmarking, it immediately sends 4 requests and then, as soon as any request is completed, it sends a new one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4294ea4-e0a6-4cb8-ab98-492374220b59",
   "metadata": {},
   "source": [
    "### Benchmarking Results\n",
    "* `'time_to_first_token-avg'`, `'latency_first_token'`: Average time between when a request is sent and when its first response is received, one value per request in the benchmark. As for many following columns, the unit is specified in a separate column: `'time_to_first_token-unit'`. In our case, this is `ns`, nanoseconds, $10^{-9}$ s, but to simplify understanding we convert `'latency_first_token'` to ms, milliseconds, $10^{-3}$ s. Some additional percentiles of the same distribution are also measured. In our dataset, we see the 95th percentile p95 in `'time_to_first_token-p95'`. `min` and `max` are also measured, but are not present in our dataset.\n",
    "* `'inter_token_latency-avg'`, `'latency_per_token_decoding'`: Average time between intermediate responses for a single request divided by the number of generated tokens of the latter response, one value per response per request in benchmark\n",
    "* `'request_latency-avg'`, `'latency'`: Time between when a request is sent and when its final response is received, one value per request in the benchmark. Also referred to as end-to-end or e2e latency.\n",
    "* `'request_throughput-avg'`, `'prompts_per_s'`: Number of final responses from benchmark divided by benchmark duration.\n",
    "* `'output_token_throughput-avg'`, `'out_tokens_per_s'`: Total number of output tokens from benchmark divided by benchmark duration\n",
    "* `'num_output_token-avg'`: Average total number of output tokens of a request, one value per request in the benchmark. Note, that these are counted with a default tokenizer. It allows comparing models between each other, but the exact token count may be skewed due to the specifics of the tokenizers used in the models.\n",
    "* `'num_input_token-avg'`: Average total number of input tokens of a request, one value per request in benchmark\n",
    "\n",
    "<a id=\"skip-definitions\"></a>\n",
    "\n",
    "More details on the reasoning behind the metrics used and comparison to LLMPerf can be found in [**the NIM benchmarking guide**](https://docs.nvidia.com/nim/benchmarking/llm/latest/metrics.html). We also provide some overview of using request rate vs concurrency during the measurement phase and in the results at the end of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5f51c3",
   "metadata": {},
   "source": [
    "<br><hr>\n",
    "\n",
    "### Investigating Benchmarked Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76de68c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the imports. To expand, click on the blue vertical bar to the left\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"iframe\"\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ipywidgets import interact, fixed, IntSlider, IntText\n",
    "import ipywidgets as widgets\n",
    "import glob\n",
    "from utils.config import config\n",
    "import utils.preprocess_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9146d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 787 rows\n"
     ]
    }
   ],
   "source": [
    "df_raw = utils.preprocess_data.load_csvs_by_glob(\"dataset/nim.csv\")\n",
    "print(f\"Loaded {len(df_raw)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c95244f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.preprocess_data import preprocess_latencies_nim_pbr\n",
    "df = preprocess_latencies_nim_pbr(df_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b31ee182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['model', 'device', 'n_gpus', 'out_tokens_per_s', 'latency_first_token',\n",
       "       'latency_per_token_decoding', 'input_len', 'output_len', 'concurrency',\n",
       "       'precision', 'prompts_per_s', 'latency', 'out_tokens_per_s_per_user',\n",
       "       'TP', 'PP', 'input_output_len', 'prompts_per_s_per_gpu',\n",
       "       'prompts_per_s_per_8_gpus', 'out_tokens_per_s_per_gpu',\n",
       "       'out_tokens_per_s_per_8_gpus', 'concurrency_per_8'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8891c16f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: ['meta-llama3-8b-instruct', 'meta-llama3-70b-instruct']\n",
      "device: ['A100-SXM4-80GB', 'H100_80GB_HBM3']\n",
      "precision: ['fp16', 'fp8', 'bf16']\n",
      "TP: [2, 1, 8, 4]\n",
      "input_output_len: ['200 in → 1000 out', '2000 in → 2000 out', '200 in → 2000 out', '7000 in → 1000 out', '2000 in → 200 out', '200 in → 200 out']\n"
     ]
    }
   ],
   "source": [
    "for c in ['model', 'device', 'precision', 'TP', 'input_output_len']:\n",
    "    print(f\"{c}: {list(df[c].unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da39c0f",
   "metadata": {},
   "source": [
    "<br><hr>\n",
    "\n",
    "### **Plotting Latency vs Throughput**\n",
    "\n",
    "This plot is crucial for benchmarking. It displays the tradeoff between latency and throughput.\n",
    "\n",
    "#### **Plot Description**\n",
    "\n",
    "* The x-axis shows a unit of latency.\n",
    "* The y-axis shows a unit of throughput.\n",
    "* Each point on the plot represents a measurement taken with the same underlying model, device, and max_batch_size.\n",
    "* Points are joined with lines, sorted by TP.\n",
    "* The difference between the points in one line is the measurement concurrency.\n",
    "\n",
    "#### **Plot Interpretation**\n",
    "\n",
    "* The best points are the closest to the top-left corner.\n",
    "* A higher point indicates greater throughput.\n",
    "* A point farther to the left indicates lower latency.\n",
    "\n",
    "You will have the opportunity to explore these plots interactively. Later, a selection of plots will be provided to illustrate specific points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69a1e96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter(df, filters, index_columns, x_metric, y_metric):\n",
    "    def compare_with_nan(series, value):\n",
    "        return series.isna() if pd.isna(value) else (series == value)\n",
    "    df_measured = df.copy()\n",
    "    df_measured = df_measured[df_measured[list(filters)].eq(pd.Series(filters)).all(axis=1)]\n",
    "    index_set = list(df_measured[index_columns].groupby(index_columns, dropna=False).first().index)\n",
    "    if len(index_columns) == 1: \n",
    "        index_set = [(v,) for v in index_set]\n",
    "    index_set.reverse()\n",
    "    fig = make_subplots()\n",
    "    df_measured = df_measured.sort_values(\"concurrency\")\n",
    "    for i, index_value in enumerate(index_set):\n",
    "        index_filters = { k: v for k, v in zip(index_columns, index_value) }\n",
    "        boolean_series_list = [compare_with_nan(df_measured[col], val) for col, val in index_filters.items()]\n",
    "        df_filtered = df_measured[np.logical_and.reduce(boolean_series_list)].sort_values(by=\"concurrency\")\n",
    "        hover = [\n",
    "            \"\"\n",
    "            + f\"{row['TP']=} <br>\"\n",
    "            + f\"{row['prompts_per_s_per_8_gpus']=:.1f} <br>\"\n",
    "            + f\"{row['concurrency']=} <br>\"\n",
    "            + f\"{row['concurrency_per_8']=} <br>\"\n",
    "            + f\"{row['input_len']=} <br>\"\n",
    "            + f\"{row['output_len']=} <br>\"\n",
    "            + f\"{row['latency']=:.1f} <br>\"\n",
    "            + f\"{row['latency_first_token']=:.1f} <br>\"\n",
    "            + f\"{row['model']=} <br>\"\n",
    "            # + f\"batch scheme: {row['batch scheme']} <br>\"\n",
    "            for i, row in df_filtered .iterrows()\n",
    "        ]\n",
    "        trace = go.Scatter(\n",
    "            x = df_filtered [x_metric],\n",
    "            y = df_filtered [y_metric],\n",
    "            name = f\", \".join(f\"{k} {v}\" for k,v in index_filters.items()),\n",
    "            customdata = hover,\n",
    "            hovertemplate = \"%{customdata}\",\n",
    "            marker={\n",
    "                \"size\": 10, \n",
    "                \"color\": px.colors.qualitative.G10[i],\n",
    "                \"opacity\": 0.7\n",
    "            },\n",
    "        )\n",
    "\n",
    "        fig.add_trace(trace)\n",
    "    fig.update_xaxes(title_text=config.columns_labels[x_metric], type=\"log\")\n",
    "    fig.update_yaxes(title_text=config.columns_labels[y_metric])\n",
    "    fig.update_layout(title=f\",<br>\".join(f\"{k} {v}\" for k,v in filters.items()))\n",
    "    fig.update_layout(margin_t=150)\n",
    "    fig.update_layout(title_pad_l=40)\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "def scatter_fixed_lengths(df, model, device, input_output_len, x_metric=\"latency_first_token\", y_metric=\"prompts_per_s\"):\n",
    "    filters = {\n",
    "        \"model\": model,\n",
    "        \"device\": device,\n",
    "        \"input_output_len\": input_output_len,\n",
    "    }\n",
    "    index_columns = [\"TP\", \"precision\"]\n",
    "    return scatter(df, filters, index_columns, x_metric, y_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e809519",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dea28d7395a6471d984d537cea0451ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='model', options=('meta-llama3-8b-instruct', 'meta-llama3-70b-instr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.scatter_fixed_lengths(df, model, device, input_output_len, x_metric='latency_first_token', y_metric='prompts_per_s')>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interact(\n",
    "    scatter_fixed_lengths, \n",
    "    df = fixed(df),\n",
    "    model = df['model'].unique(),\n",
    "    device = df['device'].unique(),\n",
    "    input_output_len = df['input_output_len'].unique(),\n",
    "    x_metric = [\"latency_per_token_decoding\", \"latency_first_token\", \"latency\"],\n",
    "    y_metric = [\"out_tokens_per_s\", \"prompts_per_s\", \"prompts_per_s_per_8_gpus\", \"out_tokens_per_s_per_user\", \"out_tokens_per_s_per_8_gpus\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3e37f1",
   "metadata": {},
   "source": [
    "<br><hr>\n",
    "\n",
    "### Latency vs Throughput Tradeoff\n",
    "\n",
    "The plots above illustrate a crucial tradeoff: **To achieve low latency for each individual request, you must compromise on throughput and vice versa.**\n",
    "\n",
    "#### Example\n",
    "* With concurrency set to 250, throughput can be up to 50 times higher than with concurrency set to 1.\n",
    "* Meanwhile, latency is only 5 times higher.\n",
    "* By relaxing latency requirements, you can significantly improve throughput and reduce Total Cost of Ownership (TCO)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caad1fb0-9625-4185-8b8d-ee30ea1a54c0",
   "metadata": {},
   "source": [
    "\n",
    "<br><hr>\n",
    "\n",
    "## **Tensor Parallelism**\n",
    "\n",
    "LLM scaling laws suggest you should expect larger and larger LLMs.\n",
    "To accommodate them, TP plays a crucial role. We partition such models across multiple GPUs. Even if a model fits on a single GPU, increasing the number of GPUs can provide benefits.\n",
    "\n",
    "#### **Benefits of Tensor Parallelism N**\n",
    "\n",
    "* N times lower memory footprint per GPU.\n",
    "* N times increase in memory bandwidth.\n",
    "* N times compute resources for the model.\n",
    "* Same precision, same accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0935a1fe-f754-4234-bce3-d0354ab0dbb9",
   "metadata": {},
   "source": [
    "#### **Scenario:** 1xTP2 vs 2xTP1\n",
    "\n",
    "If a model fits on one GPU, deploying it in TP2 mode with 2 GPUs doubles the memory bandwidth and compute resources. This reduction in latency is particularly beneficial for individual requests without batching.\n",
    "\n",
    "Running a batch of 2 requests in TP2 mode provides similar resources to running batch size 1 in 2 instances of TP1. However, TP2 mode incurs additional overhead due to communication between GPUs.\n",
    "\n",
    "Measurements highlight the importance of a low-latency GPU interconnect for inference speeds of larger models. For optimal performance, consider using:\n",
    "\n",
    "* NVLink-enabled servers like DGXes and HGXes\n",
    "* Pair-wise connected PCIe cards like H100 NVL\n",
    "\n",
    "The corresponding data has been provided for further exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62ca4641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_8.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scatter(df, {\n",
    "        \"model\": \"meta-llama3-8b-instruct\",\n",
    "        \"precision\": \"fp8\",\n",
    "        \"input_len\": 7000,\n",
    "        \"device\": 'H100_80GB_HBM3',\n",
    "    }, [\"TP\"],\n",
    "    x_metric=\"latency_first_token\",\n",
    "    y_metric=\"prompts_per_s_per_8_gpus\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ec93c3",
   "metadata": {},
   "source": [
    "Achieving the lowest possible Time-To-First-Token (TTFT) often requires a higher TP. However, if your acceptable latency is already met with a lower TP, you can potentially achieve better throughputs without sacrificing performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f662685-dfa7-4e17-a2b1-8c0b5f039257",
   "metadata": {},
   "source": [
    "#### **Scenario:** Fluctuating Tokens-Per-Second\n",
    "The plot below demonstrates that even for a single model, TP, and precision, the throughput (measured in `output_tokens/second/model_instance`) is not constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1dad04d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_9.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scatter(df, {\n",
    "        \"model\": \"meta-llama3-8b-instruct\",\n",
    "        \"precision\": \"fp16\",\n",
    "        \"TP\": 1,\n",
    "        \"device\": 'H100_80GB_HBM3',\n",
    "    }, [\"input_output_len\"],\n",
    "    x_metric=\"latency\",\n",
    "    y_metric=\"out_tokens_per_s\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5626ad4",
   "metadata": {},
   "source": [
    "When measuring throughput, it's crucial to consider the accompanying measurement parameters to ensure accurate and meaningful results:\n",
    "\n",
    "* Input and output length\n",
    "* GPU specifications\n",
    "* TP (tensor parallelism) settings\n",
    "* Concurrency levels\n",
    "* Precision settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84dde32f-a38c-4692-a4d6-c5643aebf149",
   "metadata": {},
   "source": [
    "#### **Scenario:** TTFT Incorporated Into Request\n",
    "The ratio between TTFT and E2E Latency is an important consideration. A lower ratio indicates that implementing streaming as part of the app is even more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bda0fe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_10.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df[\"ttft_ratio\"] = df[\"latency_first_token\"] / df[\"latency\"]\n",
    "scatter(df, {\n",
    "        \"model\": \"meta-llama3-8b-instruct\",\n",
    "        \"device\": 'H100_80GB_HBM3',\n",
    "        \"precision\": \"fp16\",\n",
    "        \"TP\": 1,\n",
    "    },\n",
    "    [\"input_output_len\"],\n",
    "    x_metric=\"latency\",\n",
    "    y_metric=\"ttft_ratio\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93596fb1",
   "metadata": {},
   "source": [
    "A distinctive point is recognizable in the plots where the values suddenly skyrocket. This indicates the formation of a queue. As the time in the queue grows relative to the actual request processing time, the TTFT to E2E latency ratio approaches 1. This is because the time spent in the queue is included in the TTFT measurement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50d7060-91a7-4767-9bfe-c6832d65910f",
   "metadata": {},
   "source": [
    "#### **Scenario:** FP8 vs FP16\n",
    "NVIDIA GPUs have been supporting FP8 precision since the Hopper generation, as noted in the [**Hopper whitepaper**](https://resources.nvidia.com/en-us-tensor-core), page 23, Hopper FP8 Data Format. This halves data storage requirements and doubles throughput when compared to FP16. \n",
    "\n",
    "**The key benefits of FP8-precision use include:**\n",
    "\n",
    "* **Reduced Storage Requirements:** FP8 requires less storage space compared to FP16. \n",
    "* **Increased Throughput:** FP8 doubles throughput, enabling faster processing. \n",
    "* **Minimized Accuracy Loss:** The Transformer Engine dynamically scales tensors to minimize accuracy loss when using FP8.\n",
    "\n",
    "In this plot below, consider the benchmarking difference between the two approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f45a756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_11.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scatter(df, {\n",
    "        \"model\": \"meta-llama3-8b-instruct\",\n",
    "        \"device\": 'H100_80GB_HBM3',\n",
    "        \"input_output_len\": '2000 in → 200 out',\n",
    "        \"TP\": 1,\n",
    "    },\n",
    "    [\"precision\"],\n",
    "    x_metric=\"latency_first_token\",\n",
    "    y_metric=\"prompts_per_s_per_8_gpus\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6972f75f",
   "metadata": {},
   "source": [
    "As you can see, FP8 is delivering around 2X the LLM throughput at the same latency. However, this comes with an accuracy drop. The decrease in accuracy from FP16 to FP8 is generally within 1 MMLU (Massive Multitask Language Understanding) point. For a more detailed analysis, refer to the following resources:\n",
    "* [**TRT-LLM blog post on quantization**](https://nvidia.github.io/TensorRT-LLM/blogs/quantization-in-TRT-LLM.html) \n",
    "* [**TRT-LLM quantization documentation**](https://nvidia.github.io/TensorRT-LLM/reference/precision.html).\n",
    "* To further measure the impact of quantization on your own dataset, consider using [**NeMo Evaluator Microservice**](https://developer.nvidia.com/docs/nemo-microservices/evaluation/source/overview.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60d5601",
   "metadata": {},
   "source": [
    "<br><hr>\n",
    "\n",
    "## **[EXERCISE] A Use-Case-Based Sizing Example**\n",
    "\n",
    "Explore how to use throughput vs. latency tradeoff plots to answer the sizing question with a real-world example: a call center [**RAG Customer Assistant**](https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/).\n",
    "\n",
    "**Let's assume the following:**\n",
    "* We have 30M calls per year\n",
    "* Each call takes 10 minutes\n",
    "* We call a RAG system every minute during the call\n",
    "* Each RAG system call results in 3 requests to llm\n",
    "* The peak number of requests per second is 7X of the average one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e37345a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requests_per_second_avg  = 29\n",
      "requests_per_second_peak = 200\n"
     ]
    }
   ],
   "source": [
    "calls_per_year = 30_000_000\n",
    "requests_per_call = 10 * 3\n",
    "seconds_in_year = 365 * 24 * 60 * 60\n",
    "\n",
    "requests_per_year = requests_per_call * calls_per_year\n",
    "requests_per_second_avg = requests_per_year / seconds_in_year\n",
    "requests_per_second_peak = 7 * requests_per_second_avg\n",
    "\n",
    "print(f\"{requests_per_second_avg  = :.0f}\")\n",
    "print(f\"{requests_per_second_peak = :.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f10da9d-4c30-4dcc-a337-e9752ebe56b5",
   "metadata": {},
   "source": [
    "Now, let's recap the presented list of the inputs, required for appropriate sizing and have some answers to them.\n",
    "\n",
    "* **What model are you planning to use?**\n",
    "    - Llama3-8B\n",
    "* **What is the average number of tokens in the prompt to your LLM (Length of input)?**\n",
    "    - 2000 tokens in\n",
    "* **What is the average number of tokens in your LLM output?**\n",
    "    - 200 tokens out\n",
    "* **How many requests per second should your full system process at its peak?**\n",
    "    - 200 requests per second\n",
    "* **What is your latency limit? First-token? Last-token?**\n",
    "    - 300 ms FTL. We synthesize speech as a stream.\n",
    "* **What GPUs are you considering?**\n",
    "    - As we're planning for high-load scenarios, we will benefit from utilizing multiple H100s with their price/perf ratio.\n",
    "\n",
    "To size for this use case, let's plan to use our favorite plot of throughput in \"prompts/second/8 GPUs\" vs \"first-token latency\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2faaca69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_13.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scatter_fixed_lengths(df,\n",
    "    model = 'meta-llama3-8b-instruct',\n",
    "    device = 'H100_80GB_HBM3',\n",
    "    input_output_len = '2000 in → 200 out',\n",
    "    x_metric = \"latency_first_token\",\n",
    "    y_metric =  \"prompts_per_s_per_8_gpus\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a771be5",
   "metadata": {},
   "source": [
    "Let's select a point that meets our criteria and focus on the relevant parameters. We can see from the plot that the most efficient full deployment will consist of 8 instances of the same model, deployed on one DGX H100 server with 8 H100 GPUs. By hovering over the top point on the curve with TTFT below 300ms, you will see the details from the benchmarking results.\n",
    "\n",
    "<details>\n",
    "<summary><b>Reveal Solution</b></summary>\n",
    "\n",
    "<img src=\"images/hover.png\" alt=\"hover\" width=1200px/>\n",
    "\n",
    "```\n",
    "concurrency=10\n",
    "prompts_per_second_per_8_gpus=41.6\n",
    "```\n",
    "\n",
    "</details>\n",
    "<br>\n",
    "\n",
    "Knowing the `concurrency` and `prompts_per_second_per_(8_gpus AKA dgx)`, we can compute the number of DGX units needed to make up our target system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d9502c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Latency of ~220ms: 0.0 DGX Units\n",
      "Target Latency of ~500ms: 0.0 DGX Units\n"
     ]
    }
   ],
   "source": [
    "## TODO: Calculate target target system/8gpu = number of dgxs\n",
    "def get_num_dgxs(target_prompt_per_second, prompts_per_second_per_dgx):\n",
    "    \"\"\"\n",
    "    arg1: target prompts/second/sys\n",
    "    arg2: prompts/second/8gpu\n",
    "    return: target system/8gpu\n",
    "    \"\"\"\n",
    "    return 0\n",
    "\n",
    "peak_throughput = requests_per_second_peak # = 200\n",
    "## TODO: Retrieve statistic from table for a sufficient first-token latency\n",
    "throughput_per_dgx_h100_220ms = 0  ## TODO: Populate for TTFT ~ 220ms\n",
    "throughput_per_dgx_h100_500ms = 0  ## TODO: Populate for TTFT ~ 500ms\n",
    "\n",
    "num_dgxs_220ms = get_num_dgxs(peak_throughput, throughput_per_dgx_h100_220ms)\n",
    "num_dgxs_500ms = get_num_dgxs(peak_throughput, throughput_per_dgx_h100_500ms)\n",
    "\n",
    "print(f\"Target Latency of ~220ms: {num_dgxs_220ms:.1f} DGX Units\")\n",
    "print(f\"Target Latency of ~500ms: {num_dgxs_500ms:.1f} DGX Units\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3844bb3c-2196-4367-862f-d9ad3a0d72dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Latency of ~220ms: 4.8 DGX Units\n",
      "Target Latency of ~500ms: 2.8 DGX Units\n"
     ]
    }
   ],
   "source": [
    "def get_num_dgxs(target_prompt_per_second, prompts_per_second_per_dgx):\n",
    "    \"\"\"\n",
    "    arg1: target prompts/second/sys\n",
    "    arg2: prompts/second/8gpu\n",
    "    return: target system/8gpu\n",
    "    \"\"\"\n",
    "    return target_prompt_per_second / prompts_per_second_per_dgx\n",
    "\n",
    "peak_throughput = requests_per_second_peak # = 200\n",
    "## TODO: Retrieve statistic from table for a sufficient first-token latency\n",
    "throughput_per_dgx_h100_220ms = 41.6\n",
    "throughput_per_dgx_h100_500ms = 70.7\n",
    "\n",
    "num_dgxs_220ms = get_num_dgxs(peak_throughput, throughput_per_dgx_h100_220ms)\n",
    "num_dgxs_500ms = get_num_dgxs(peak_throughput, throughput_per_dgx_h100_500ms)\n",
    "\n",
    "print(f\"Target Latency of ~220ms: {num_dgxs_220ms:.1f} DGX Units\")\n",
    "print(f\"Target Latency of ~500ms: {num_dgxs_500ms:.1f} DGX Units\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c773ae2",
   "metadata": {},
   "source": [
    "So it looks like for this use case, we will need 4.8 DGX systems to meet our target latency requirement based on our closest benchmark point (220ms), and could intuit that 4 would likely fall a bit short while still being comparable. Notice also how a shift to just 500ms TTFT would move that number all the way down to 2.8 DGX systems for the same number of customers.  \n",
    "\n",
    "<details>\n",
    "<summary><b>Reveal Solution</b></summary>\n",
    "\n",
    "```python \n",
    "## TODO: Calculate target target system/8gpu = number of dgxs\n",
    "def get_num_dgxs(target_prompt_per_second, prompts_per_second_per_dgx):\n",
    "    \"\"\"\n",
    "    arg1: target prompts/second/sys\n",
    "    arg2: prompts/second/8gpu\n",
    "    return: target system/8gpu\n",
    "    \"\"\"\n",
    "    return target_prompt_per_second / prompts_per_second_per_dgx\n",
    "\n",
    "peak_throughput = requests_per_second_peak # = 200\n",
    "## TODO: Retrieve statistic from table for a sufficient first-token latency\n",
    "throughput_per_dgx_h100_220ms = 41.6\n",
    "throughput_per_dgx_h100_500ms = 70.7\n",
    "\n",
    "num_dgxs_220ms = get_num_dgxs(peak_throughput, throughput_per_dgx_h100_220ms)\n",
    "num_dgxs_500ms = get_num_dgxs(peak_throughput, throughput_per_dgx_h100_500ms)\n",
    "\n",
    "print(f\"Target Latency of ~220ms: {num_dgxs_220ms:.1f} DGX Units\")\n",
    "print(f\"Target Latency of ~500ms: {num_dgxs_500ms:.1f} DGX Units\")\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3338638",
   "metadata": {},
   "source": [
    "<hr><br>\n",
    "\n",
    "## **Sizing Best Practices**\n",
    "To effectively estimate sizing for your project, the following best practices have been identified:\n",
    "\n",
    "#### NVIDIA Software Stack\n",
    "\n",
    "* Use the NVIDIA software stack, including: [NIM](https://docs.nvidia.com/nim/large-language-models/latest/getting-started.html), [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM/tree/main) and [Triton Inference Server](https://github.com/triton-inference-server).\n",
    "* Utilize [GenAI-Perf](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/client/src/c%2B%2B/perf_analyzer/genai-perf/README.html) and [the NIM benchmarking guide](https://docs.nvidia.com/nim/benchmarking/llm/latest/metrics.html) to measure the performance.\n",
    "\n",
    "#### Model Size and Hardware Considerations\n",
    "\n",
    "* For models larger than 13B that require more than one GPU, prefer NVLink-enabled systems.\n",
    "* Larger models require more memory and have higher latency, scaling approximately with the model size.\n",
    "\n",
    "#### Streaming vs. Sequential Mode\n",
    "\n",
    "* Apps where the LLM response is consumed directly should be developed in streaming mode.\n",
    "* Apps where the LLM response is consumed by another LLM or sequential system require sequential mode.\n",
    "* When designing apps from scratch, develop them to rely on streaming mode. For older apps, sequential mode may be used as a workaround to introduce LLMs.\n",
    "\n",
    "#### Other Performance Considerations\n",
    "\n",
    "* The cost and latency are usually dominated by the number of output tokens, as indicated by `ttft_ratio`. Input tokens are much cheaper.\n",
    "* TTFT is determined by the input length in streaming mode.\n",
    "* Generating text is almost always faster than human reading speed.\n",
    "* Introducing strict latency limits can significantly decrease available throughput."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04b7587-4510-456d-a6b1-e70794a9d357",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
